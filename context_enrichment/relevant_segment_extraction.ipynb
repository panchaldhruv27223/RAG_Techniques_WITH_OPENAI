{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7697c429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74a4c157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b89fa012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\TempAccess\\\\Documents\\\\Dhruv\\\\RAG'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(r\"C:\\Users\\TempAccess\\Documents\\Dhruv\\RAG\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a49bee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_function_openai import (\n",
    "    Document,\n",
    "    RetrievalResult,\n",
    "    OpenAIEmbedder,\n",
    "    FAISSVectorStore,\n",
    "    OpenAIChat,\n",
    "    read_pdf,\n",
    "    chunk_text,\n",
    "    cosine_similarity,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc39ddec",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ChunkMeta:\n",
    "    \"\"\"\n",
    "    Metadata for a single chunk stored in the key-value store.\n",
    "\n",
    "    Attributes:\n",
    "        doc_id:       Identifier for the source document.\n",
    "        chunk_index:  Position of this chunk within the document (0-based).\n",
    "        text:         Raw text content of the chunk.\n",
    "    \"\"\"\n",
    "    doc_id: str\n",
    "    chunk_index: int\n",
    "    text: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Segment:\n",
    "    \"\"\"\n",
    "    A contiguous segment of text reconstructed from multiple chunks.\n",
    "\n",
    "    Attributes:\n",
    "        doc_id:       Source document identifier.\n",
    "        start_index:  Start chunk index (inclusive).\n",
    "        end_index:    End chunk index (exclusive).\n",
    "        text:         Concatenated text of all chunks in [start_index, end_index).\n",
    "        score:        Segment-level relevance score.\n",
    "    \"\"\"\n",
    "    doc_id: str\n",
    "    start_index: int\n",
    "    end_index: int\n",
    "    text: str\n",
    "    score: float\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2a69d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _beta_cdf_approx(x: float, a: float = 0.4, b: float = 0.4) -> float:\n",
    "    \"\"\"\n",
    "    Approximate Beta CDF using a simple numerical integration.\n",
    "\n",
    "    The original notebook uses scipy.stats.beta.cdf to spread out\n",
    "    the relevance values more uniformly. This implementation avoids\n",
    "    the scipy dependency by using a basic trapezoidal integration\n",
    "    over the Beta PDF: f(t) = t^(a-1) * (1-t)^(b-1) / B(a,b).\n",
    "\n",
    "    Args:\n",
    "        x:  Value in [0, 1].\n",
    "        a:  Alpha parameter of the Beta distribution.\n",
    "        b:  Beta parameter of the Beta distribution.\n",
    "\n",
    "    Returns:\n",
    "        Approximate CDF value at x.\n",
    "    \"\"\"\n",
    "    if x <= 0.0:\n",
    "        return 0.0\n",
    "    if x >= 1.0:\n",
    "        return 1.0\n",
    "\n",
    "    # Compute Beta function B(a, b) = Gamma(a)*Gamma(b)/Gamma(a+b)\n",
    "    beta_func = math.gamma(a) * math.gamma(b) / math.gamma(a + b)\n",
    "\n",
    "    # Trapezoidal integration from 0 to x\n",
    "    n_steps = 200\n",
    "    dt = x / n_steps\n",
    "    total = 0.0\n",
    "    for i in range(n_steps + 1):\n",
    "        t = i * dt\n",
    "        # Clamp t to avoid 0^negative\n",
    "        t_clamped = max(t, 1e-12)\n",
    "        one_minus_t = max(1.0 - t, 1e-12)\n",
    "        val = (t_clamped ** (a - 1)) * (one_minus_t ** (b - 1))\n",
    "        # Trapezoidal rule: half-weight at endpoints\n",
    "        if i == 0 or i == n_steps:\n",
    "            total += val * 0.5\n",
    "        else:\n",
    "            total += val\n",
    "    total *= dt\n",
    "\n",
    "    return total / beta_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39563e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkKVStore:\n",
    "    \"\"\"\n",
    "    In-memory key-value store for chunk text, keyed by (doc_id, chunk_index).\n",
    "\n",
    "    RSE needs to retrieve chunks that weren't in the initial search results\n",
    "    (e.g. non-relevant chunks sandwiched between relevant ones). This store\n",
    "    enables O(1) lookup by position.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._store: Dict[Tuple[str, int], ChunkMeta] = {}\n",
    "        self._doc_lengths: Dict[str, int] = {}  # doc_id → total chunks\n",
    "\n",
    "    def add(self, doc_id: str, chunk_index: int, text: str) -> None:\n",
    "        key = (doc_id, chunk_index)\n",
    "        self._store[key] = ChunkMeta(doc_id=doc_id, chunk_index=chunk_index, text=text)\n",
    "        # Track max chunk index per document\n",
    "        self._doc_lengths[doc_id] = max(\n",
    "            self._doc_lengths.get(doc_id, 0), chunk_index + 1\n",
    "        )\n",
    "\n",
    "    def get(self, doc_id: str, chunk_index: int) -> Optional[ChunkMeta]:\n",
    "        return self._store.get((doc_id, chunk_index))\n",
    "\n",
    "    def get_segment_text(self, doc_id: str, start: int, end: int) -> str:\n",
    "        \"\"\"\n",
    "        Reconstruct contiguous text for chunks [start, end).\n",
    "\n",
    "        Args:\n",
    "            doc_id:  Document identifier.\n",
    "            start:   Start chunk index (inclusive).\n",
    "            end:     End chunk index (exclusive).\n",
    "\n",
    "        Returns:\n",
    "            Concatenated text of all chunks in the range.\n",
    "        \"\"\"\n",
    "        parts = []\n",
    "        for i in range(start, end):\n",
    "            meta = self.get(doc_id, i)\n",
    "            if meta:\n",
    "                parts.append(meta.text)\n",
    "        return \"\\n\".join(parts)\n",
    "\n",
    "    def doc_chunk_count(self, doc_id: str) -> int:\n",
    "        return self._doc_lengths.get(doc_id, 0)\n",
    "\n",
    "    @property\n",
    "    def all_doc_ids(self) -> List[str]:\n",
    "        return list(self._doc_lengths.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8745516",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAIReranker:\n",
    "    \"\"\"\n",
    "    Reranks chunks by computing cosine similarity between the query\n",
    "    embedding and each chunk embedding via OpenAI's embedding model.\n",
    "\n",
    "    This replaces the Cohere reranker used in the original notebook.\n",
    "    While a dedicated cross-encoder reranker may perform better, this\n",
    "    embedding-based approach keeps the dependency footprint minimal\n",
    "    (OpenAI only) and still provides meaningful relevance scores.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedder: OpenAIEmbedder):\n",
    "        self.embedder = embedder\n",
    "\n",
    "    def rerank(\n",
    "        self,\n",
    "        query: str,\n",
    "        chunks: List[str],\n",
    "        decay_rate: float = 30.0,\n",
    "    ) -> Tuple[List[float], List[float]]:\n",
    "        \"\"\"\n",
    "        Compute similarity scores and chunk values for all chunks.\n",
    "\n",
    "        Args:\n",
    "            query:       Search query string.\n",
    "            chunks:      List of chunk texts to score.\n",
    "            decay_rate:  Exponential decay rate applied to ranks.\n",
    "\n",
    "        Returns:\n",
    "            similarity_scores:  List of transformed absolute relevance (0–1), \n",
    "                                in original document order.\n",
    "            chunk_values:       List of relevance values fusing rank + similarity,\n",
    "                                in original document order.\n",
    "        \"\"\"\n",
    "        if not chunks:\n",
    "            return [], []\n",
    "\n",
    "        # Embed query and all chunks\n",
    "        query_emb = self.embedder.embed_text(query)\n",
    "        chunk_embs = self.embedder.embed_texts(chunks)\n",
    "\n",
    "        # Compute raw cosine similarities\n",
    "        raw_scores = []\n",
    "        for emb in chunk_embs:\n",
    "            sim = cosine_similarity(query_emb, emb)\n",
    "            # Clamp to [0, 1] for the beta transform\n",
    "            sim = max(0.0, min(1.0, (sim + 1.0) / 2.0))  # normalize from [-1,1] to [0,1]\n",
    "            raw_scores.append(sim)\n",
    "\n",
    "        # Sort by score descending to get ranks\n",
    "        indexed_scores = list(enumerate(raw_scores))\n",
    "        indexed_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # Build output arrays in original order\n",
    "        similarity_scores = [0.0] * len(chunks)\n",
    "        chunk_values = [0.0] * len(chunks)\n",
    "\n",
    "        for rank, (orig_idx, raw_sim) in enumerate(indexed_scores):\n",
    "            transformed = _beta_cdf_approx(raw_sim)\n",
    "            similarity_scores[orig_idx] = transformed\n",
    "            # Fuse rank-based decay with absolute relevance\n",
    "            chunk_values[orig_idx] = math.exp(-rank / decay_rate) * transformed\n",
    "\n",
    "        return similarity_scores, chunk_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2918a145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_segments(\n",
    "    relevance_values: List[float],\n",
    "    max_length: int,\n",
    "    overall_max_length: int,\n",
    "    minimum_value: float,\n",
    ") -> Tuple[List[Tuple[int, int]], List[float]]:\n",
    "    \"\"\"\n",
    "    Find the best non-overlapping contiguous segments by solving a\n",
    "    constrained version of the maximum-sum-subarray problem.\n",
    "\n",
    "    After subtracting the irrelevant_chunk_penalty, irrelevant chunks have\n",
    "    negative values and relevant chunks have positive values. Segment value\n",
    "    is the sum of its constituent chunk values. The algorithm greedily picks\n",
    "    the best segment, then repeats until constraints are met.\n",
    "\n",
    "    Args:\n",
    "        relevance_values:   Per-chunk values (already penalty-adjusted).\n",
    "        max_length:         Max number of chunks in a single segment.\n",
    "        overall_max_length: Max total chunks across all segments.\n",
    "        minimum_value:      Minimum score a segment must have to be kept.\n",
    "\n",
    "    Returns:\n",
    "        best_segments:  List of (start, end) tuples (end is exclusive).\n",
    "        scores:         Corresponding segment scores.\n",
    "    \"\"\"\n",
    "    best_segments: List[Tuple[int, int]] = []\n",
    "    scores: List[float] = []\n",
    "    total_length = 0\n",
    "\n",
    "    while total_length < overall_max_length:\n",
    "        best_segment = None\n",
    "        best_value = -1000.0\n",
    "\n",
    "        for start in range(len(relevance_values)):\n",
    "            # Skip negative starting points\n",
    "            if relevance_values[start] < 0:\n",
    "                continue\n",
    "\n",
    "            for end in range(\n",
    "                start + 1,\n",
    "                min(start + max_length + 1, len(relevance_values) + 1),\n",
    "            ):\n",
    "                # Skip negative ending points\n",
    "                if relevance_values[end - 1] < 0:\n",
    "                    continue\n",
    "\n",
    "                # Check overlap with existing segments\n",
    "                if any(\n",
    "                    start < seg_end and end > seg_start\n",
    "                    for seg_start, seg_end in best_segments\n",
    "                ):\n",
    "                    continue\n",
    "\n",
    "                # Check overall length constraint\n",
    "                if total_length + (end - start) > overall_max_length:\n",
    "                    continue\n",
    "\n",
    "                segment_value = sum(relevance_values[start:end])\n",
    "                if segment_value > best_value:\n",
    "                    best_value = segment_value\n",
    "                    best_segment = (start, end)\n",
    "\n",
    "        # No valid segment found or below minimum\n",
    "        if best_segment is None or best_value < minimum_value:\n",
    "            break\n",
    "\n",
    "        best_segments.append(best_segment)\n",
    "        scores.append(best_value)\n",
    "        total_length += best_segment[1] - best_segment[0]\n",
    "\n",
    "    return best_segments, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94864bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSERetriever:\n",
    "    \"\"\"\n",
    "    Retriever that uses Relevant Segment Extraction (RSE) to return\n",
    "    contiguous, multi-chunk segments instead of isolated top-k chunks.\n",
    "\n",
    "    Pipeline:\n",
    "        1. Index document with zero-overlap chunking\n",
    "        2. Store every chunk in both FAISS vector index + key-value store\n",
    "        3. On query: retrieve top-k candidates via vector search\n",
    "        4. Rerank ALL chunks of candidate documents using OpenAI embeddings\n",
    "        5. Compute chunk values (similarity × rank decay − threshold)\n",
    "        6. Run segment optimization per document\n",
    "        7. Return reconstructed segment texts\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model: str = \"text-embedding-3-small\",\n",
    "        chunk_size: int = 800,\n",
    "        irrelevant_chunk_penalty: float = 0.2,\n",
    "        max_segment_length: int = 20,\n",
    "        overall_max_length: int = 30,\n",
    "        minimum_segment_value: float = 0.7,\n",
    "        decay_rate: float = 30.0,\n",
    "        initial_k: int = 40,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_model:          OpenAI embedding model.\n",
    "            chunk_size:               Characters per chunk (NO overlap for RSE).\n",
    "            irrelevant_chunk_penalty: Subtracted from every chunk value; controls\n",
    "                                      how aggressively irrelevant chunks are penalized.\n",
    "                                      ~0.2 works well empirically.\n",
    "            max_segment_length:       Max chunks in one segment.\n",
    "            overall_max_length:       Max total chunks across all returned segments.\n",
    "            minimum_segment_value:    Min score for a segment to be returned.\n",
    "            decay_rate:               Exponential decay rate for rank-based scoring.\n",
    "            initial_k:                Number of candidates from initial vector search.\n",
    "        \"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.irrelevant_chunk_penalty = irrelevant_chunk_penalty\n",
    "        self.max_segment_length = max_segment_length\n",
    "        self.overall_max_length = overall_max_length\n",
    "        self.minimum_segment_value = minimum_segment_value\n",
    "        self.decay_rate = decay_rate\n",
    "        self.initial_k = initial_k\n",
    "\n",
    "        # Core components\n",
    "        self.embedder = OpenAIEmbedder(model=embedding_model)\n",
    "        self.vector_store = FAISSVectorStore(dimension=self.embedder.dimension)\n",
    "        self.kv_store = ChunkKVStore()\n",
    "        self.reranker = OpenAIReranker(self.embedder)\n",
    "\n",
    "    def index_document(self, text: str, doc_id: str = \"doc_0\") -> int:\n",
    "        \"\"\"\n",
    "        Chunk and index a single document.\n",
    "\n",
    "        IMPORTANT: RSE requires chunk_overlap=0 so that chunks can be\n",
    "        cleanly concatenated to reconstruct document segments.\n",
    "\n",
    "        Args:\n",
    "            text:    Full document text.\n",
    "            doc_id:  Unique identifier for this document.\n",
    "\n",
    "        Returns:\n",
    "            Number of chunks created.\n",
    "        \"\"\"\n",
    "        # Split with ZERO overlap (RSE requirement)\n",
    "        chunks = chunk_text(text, chunk_size=self.chunk_size, chunk_overlap=0)\n",
    "\n",
    "        documents = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Store in key-value store\n",
    "            self.kv_store.add(doc_id, i, chunk)\n",
    "\n",
    "            # Prepare for vector indexing\n",
    "            documents.append(\n",
    "                Document(\n",
    "                    content=chunk,\n",
    "                    metadata={\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"chunk_index\": i,\n",
    "                        \"total_chunks\": len(chunks),\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Embed and add to FAISS\n",
    "        documents = self.embedder.embed_documents(documents)\n",
    "        self.vector_store.add_documents(documents)\n",
    "\n",
    "        return len(chunks)\n",
    "\n",
    "    def index_pdf(self, file_path: str, doc_id: Optional[str] = None) -> int:\n",
    "        \"\"\"\n",
    "        Read a PDF and index its contents.\n",
    "\n",
    "        Args:\n",
    "            file_path:  Path to PDF file.\n",
    "            doc_id:     Document ID (defaults to filename).\n",
    "\n",
    "        Returns:\n",
    "            Number of chunks created.\n",
    "        \"\"\"\n",
    "        if doc_id is None:\n",
    "            doc_id = os.path.basename(file_path)\n",
    "\n",
    "        text = read_pdf(file_path)\n",
    "        return self.index_document(text, doc_id)\n",
    "\n",
    "    def index_text_file(self, file_path: str, doc_id: Optional[str] = None) -> int:\n",
    "        \"\"\"\n",
    "        Read a text file and index its contents.\n",
    "\n",
    "        Args:\n",
    "            file_path:  Path to text file.\n",
    "            doc_id:     Document ID (defaults to filename).\n",
    "\n",
    "        Returns:\n",
    "            Number of chunks created.\n",
    "        \"\"\"\n",
    "        if doc_id is None:\n",
    "            doc_id = os.path.basename(file_path)\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        return self.index_document(text, doc_id)\n",
    "\n",
    "    def retrieve_segments(self, query: str) -> List[Segment]:\n",
    "        \"\"\"\n",
    "        Core RSE retrieval pipeline.\n",
    "\n",
    "        Steps:\n",
    "            1. Vector search to find candidate chunks\n",
    "            2. Identify which documents contain those candidates\n",
    "            3. For each candidate document, rerank ALL its chunks\n",
    "            4. Compute chunk values (fused relevance − penalty)\n",
    "            5. Find optimal segments via constrained max-subarray\n",
    "            6. Reconstruct and return segment texts\n",
    "\n",
    "        Args:\n",
    "            query: Search query string.\n",
    "\n",
    "        Returns:\n",
    "            List of Segment objects, sorted by score descending.\n",
    "        \"\"\"\n",
    "        # Step 1: Initial vector search for candidates\n",
    "        query_emb = self.embedder.embed_text(query)\n",
    "        results = self.vector_store.search(query_emb, k=self.initial_k)\n",
    "\n",
    "        if not results:\n",
    "            return []\n",
    "\n",
    "        # Step 2: Identify candidate documents\n",
    "        candidate_doc_ids = set()\n",
    "        for r in results:\n",
    "            doc_id = r.document.metadata.get(\"doc_id\", \"unknown\")\n",
    "            candidate_doc_ids.add(doc_id)\n",
    "\n",
    "        # Step 3–5: Process each candidate document\n",
    "        all_segments: List[Segment] = []\n",
    "\n",
    "        for doc_id in candidate_doc_ids:\n",
    "            num_chunks = self.kv_store.doc_chunk_count(doc_id)\n",
    "            if num_chunks == 0:\n",
    "                continue\n",
    "\n",
    "            # Gather all chunk texts for this document\n",
    "            doc_chunks = []\n",
    "            for i in range(num_chunks):\n",
    "                meta = self.kv_store.get(doc_id, i)\n",
    "                doc_chunks.append(meta.text if meta else \"\")\n",
    "\n",
    "            # Rerank all chunks against the query\n",
    "            _, chunk_values = self.reranker.rerank(\n",
    "                query, doc_chunks, decay_rate=self.decay_rate\n",
    "            )\n",
    "\n",
    "            # Subtract threshold to penalize irrelevant chunks\n",
    "            adjusted_values = [\n",
    "                v - self.irrelevant_chunk_penalty for v in chunk_values\n",
    "            ]\n",
    "\n",
    "            # Find best segments\n",
    "            segments, scores = get_best_segments(\n",
    "                adjusted_values,\n",
    "                max_length=self.max_segment_length,\n",
    "                overall_max_length=self.overall_max_length,\n",
    "                minimum_value=self.minimum_segment_value,\n",
    "            )\n",
    "\n",
    "            # Reconstruct segment text\n",
    "            for (start, end), score in zip(segments, scores):\n",
    "                text = self.kv_store.get_segment_text(doc_id, start, end)\n",
    "                all_segments.append(\n",
    "                    Segment(\n",
    "                        doc_id=doc_id,\n",
    "                        start_index=start,\n",
    "                        end_index=end,\n",
    "                        text=text,\n",
    "                        score=score,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # Sort all segments by score descending\n",
    "        all_segments.sort(key=lambda s: s.score, reverse=True)\n",
    "        return all_segments\n",
    "\n",
    "    def retrieve_context(self, query: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Convenience method: retrieve segment texts as a list of strings.\n",
    "\n",
    "        Args:\n",
    "            query: Search query.\n",
    "\n",
    "        Returns:\n",
    "            List of segment text strings.\n",
    "        \"\"\"\n",
    "        segments = self.retrieve_segments(query)\n",
    "        return [seg.text for seg in segments]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c67e1b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSERetrievalRAG:\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline using Relevant Segment Extraction.\n",
    "\n",
    "    Combines RSERetriever (for intelligent multi-chunk retrieval) with\n",
    "    OpenAIChat (for answer generation). Follows the same interface\n",
    "    pattern as SimpleRAGOpenai and other RAG classes in this project.\n",
    "\n",
    "    Usage:\n",
    "        rag = RSERetrievalRAG(file_path=\"report.pdf\")\n",
    "        answer, segments = rag.query(\"What are the key financial metrics?\")\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_path: str,\n",
    "        chunk_size: int = 800,\n",
    "        irrelevant_chunk_penalty: float = 0.2,\n",
    "        max_segment_length: int = 20,\n",
    "        overall_max_length: int = 30,\n",
    "        minimum_segment_value: float = 0.7,\n",
    "        decay_rate: float = 30.0,\n",
    "        initial_k: int = 40,\n",
    "        embedding_model: str = \"text-embedding-3-small\",\n",
    "        chat_model: str = \"gpt-4o-mini\",\n",
    "        temperature: float = 0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the RSE RAG pipeline.\n",
    "\n",
    "        Args:\n",
    "            file_path:                Path to document (PDF or text file).\n",
    "            chunk_size:               Characters per chunk (0 overlap enforced).\n",
    "            irrelevant_chunk_penalty: Threshold subtracted from chunk values (~0.2).\n",
    "            max_segment_length:       Max chunks in one segment.\n",
    "            overall_max_length:       Max total chunks across all segments.\n",
    "            minimum_segment_value:    Min score for a segment to be returned.\n",
    "            decay_rate:               Exponential decay for rank-based scoring.\n",
    "            initial_k:                Vector search candidates.\n",
    "            embedding_model:          OpenAI embedding model.\n",
    "            chat_model:               OpenAI chat model.\n",
    "            temperature:              LLM temperature.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "\n",
    "        # Initialize retriever\n",
    "        self.retriever = RSERetriever(\n",
    "            embedding_model=embedding_model,\n",
    "            chunk_size=chunk_size,\n",
    "            irrelevant_chunk_penalty=irrelevant_chunk_penalty,\n",
    "            max_segment_length=max_segment_length,\n",
    "            overall_max_length=overall_max_length,\n",
    "            minimum_segment_value=minimum_segment_value,\n",
    "            decay_rate=decay_rate,\n",
    "            initial_k=initial_k,\n",
    "        )\n",
    "\n",
    "        # Initialize chat model\n",
    "        self.chat = OpenAIChat(\n",
    "            model_name=chat_model,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "        # Index the document\n",
    "        if file_path.endswith(\".pdf\"):\n",
    "            num_chunks = self.retriever.index_pdf(file_path)\n",
    "        else:\n",
    "            num_chunks = self.retriever.index_text_file(file_path)\n",
    "\n",
    "        print(f\"[RSE] Indexed '{os.path.basename(file_path)}' → {num_chunks} chunks (0 overlap)\")\n",
    "\n",
    "    def query(\n",
    "        self,\n",
    "        question: str,\n",
    "        return_context: bool = True,\n",
    "    ) -> Tuple[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Query the RSE RAG system.\n",
    "\n",
    "        Args:\n",
    "            question:        User's question.\n",
    "            return_context:  Whether to return retrieved segments.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (answer_string, list_of_segment_texts).\n",
    "        \"\"\"\n",
    "        # Retrieve segments\n",
    "        segments = self.retriever.retrieve_segments(question)\n",
    "        context = [seg.text for seg in segments]\n",
    "\n",
    "        if not context:\n",
    "            return \"No relevant information found in the document.\", []\n",
    "\n",
    "        # Generate answer\n",
    "        answer = self.chat.chat_with_context(question, context)\n",
    "\n",
    "        if return_context:\n",
    "            return answer, context\n",
    "        return answer, []\n",
    "\n",
    "    def show_segments(self, question: str) -> None:\n",
    "        \"\"\"\n",
    "        Debug helper: print retrieved segments with metadata.\n",
    "\n",
    "        Args:\n",
    "            question: Search query.\n",
    "        \"\"\"\n",
    "        segments = self.retriever.retrieve_segments(question)\n",
    "\n",
    "        print(f\"\\nQuery: {question}\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        if not segments:\n",
    "            print(\"  No segments found.\")\n",
    "            return\n",
    "\n",
    "        for i, seg in enumerate(segments):\n",
    "            print(f\"\\n  Segment {i + 1}:\")\n",
    "            print(f\"    Document:  {seg.doc_id}\")\n",
    "            print(f\"    Chunks:    [{seg.start_index}, {seg.end_index})\")\n",
    "            print(f\"    Length:    {seg.end_index - seg.start_index} chunks\")\n",
    "            print(f\"    Score:     {seg.score:.4f}\")\n",
    "            print(f\"    Preview:   {seg.text[:200]}...\")\n",
    "            print(\"-\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "639c6e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = r\"data\\Understanding_Climate_Change.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b5781c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RSE] Indexed 'Understanding_Climate_Change.pdf' → 95 chunks (0 overlap)\n",
      "\n",
      "[RSE RAG] Ready. Type 'exit' to quit.\n",
      "\n",
      "\n",
      "Query: what is climat change???\n",
      "======================================================================\n",
      "\n",
      "  Segment 1:\n",
      "    Document:  Understanding_Climate_Change.pdf\n",
      "    Chunks:    [29, 49)\n",
      "    Length:    20 chunks\n",
      "    Score:     61153.2425\n",
      "    Preview:   economic activities. Urban climate initiatives include sustainable transportation systems, \n",
      "green building standards, and climate-resilient infrastructure. Community engagement and \n",
      "participatory plan...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "  Segment 2:\n",
      "    Document:  Understanding_Climate_Change.pdf\n",
      "    Chunks:    [0, 10)\n",
      "    Length:    10 chunks\n",
      "    Score:     34817.3989\n",
      "    Preview:   Understanding Climate Change \n",
      "Chapter 1: Introduction to Climate Change \n",
      "Climate change refers to significant, long-term changes in the global climate. The term \n",
      "\"global climate\" encompasses the plane...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Answer: Climate change refers to significant, long-term changes in the global climate, encompassing the planet's overall weather patterns, including temperature, precipitation, and wind patterns, over an extended period. It is primarily driven by human activities, particularly the burning of fossil fuels and deforestation, which have led to an increase in greenhouse gases in the atmosphere, resulting in a warmer climate.\n",
      "Segments used: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rag = RSERetrievalRAG(\n",
    "    file_path=pdf_path,\n",
    "    chunk_size=800,\n",
    "    irrelevant_chunk_penalty=0.2,\n",
    "    max_segment_length=20,\n",
    "    overall_max_length=30,\n",
    "    minimum_segment_value=0.7,\n",
    ")\n",
    "\n",
    "# Interactive loop\n",
    "print(\"\\n[RSE RAG] Ready. Type 'exit' to quit.\\n\")\n",
    "while True:\n",
    "    question = input(\"User: \").strip()\n",
    "    if question.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    # Show segments for debugging\n",
    "    rag.show_segments(question)\n",
    "\n",
    "    # Get answer\n",
    "    answer, context = rag.query(question)\n",
    "    print(f\"\\nAnswer: {answer}\")\n",
    "    print(f\"Segments used: {len(context)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7500b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
