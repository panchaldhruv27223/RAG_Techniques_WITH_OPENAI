{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd16045d",
   "metadata": {},
   "source": [
    "![alt text](contextual_compression.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c64d5b17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9cce583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\TempAccess\\\\Documents\\\\Dhruv\\\\RAG'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(r\"C:\\Users\\TempAccess\\Documents\\Dhruv\\RAG\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8d26c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_function_openai import (\n",
    "    Document,\n",
    "    RetrievalResult,\n",
    "    OpenAIEmbedder,\n",
    "    FAISSVectorStore,\n",
    "    OpenAIChat,\n",
    "    read_pdf,\n",
    "    chunk_text,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f312c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualCompressor:\n",
    "    \"\"\"\n",
    "    Uses an LLM to extract only the query-relevant portions from a chunk.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        model_name:   OpenAI model for compression.\n",
    "        temperature:  Should be 0 for deterministic extraction.\n",
    "        max_tokens:   Max tokens for compressed output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name:str = \"gpt-4o-mini\",\n",
    "        temperature:float=0.0,\n",
    "        max_tokens:int=5000\n",
    "        ):\n",
    "\n",
    "        self.llm = OpenAIChat(\n",
    "            model_name=model_name,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens\n",
    "            )\n",
    "\n",
    "    \n",
    "    def compress(self, chunk_text:str, query:str)->str:\n",
    "        \"\"\"\n",
    "        Extract only the query-relevant portions from a chunk.\n",
    "\n",
    "        This is the core operation. The LLM reads the chunk in the context\n",
    "        of the query and returns only the relevant parts. If nothing is\n",
    "        relevant, it returns \"NO_RELEVANT_CONTENT\".\n",
    "\n",
    "        Args:\n",
    "            chunk_text:  The full retrieved chunk text.\n",
    "            query:       The user's question.\n",
    "\n",
    "        Returns:\n",
    "            Compressed/extracted text, or empty string if nothing relevant.\n",
    "        \"\"\"\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are an expert text extractor. Given a user query and a text chunk, \"\n",
    "                    \"extract ONLY the parts of the text that are directly relevant to answering \"\n",
    "                    \"the query. Preserve the original wording — do not paraphrase or summarize. \"\n",
    "                    \"If no part of the text is relevant to the query, respond with exactly: \"\n",
    "                    \"NO_RELEVANT_CONTENT\"\n",
    "                ),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"Query: {query}\\n\\n\"\n",
    "                    f\"Text chunk:\\n{chunk_text}\\n\\n\"\n",
    "                    \"Extract only the relevant parts:\"\n",
    "                ),\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        result = self.llm.chat(messages)\n",
    "\n",
    "        # Check if the LLM found nothing relevant\n",
    "        if \"NO_RELEVANT_CONTENT\" in result.strip():\n",
    "            return \"\"\n",
    "\n",
    "        return result.strip()\n",
    "\n",
    "\n",
    "    \n",
    "    def compress_document(\n",
    "        self,\n",
    "        chunks:List[str],\n",
    "        query:str,\n",
    "        )->List[str]:\n",
    "\n",
    "        \"\"\"\n",
    "        Compress multiple chunks, filtering out irrelevant ones.\n",
    "\n",
    "        Args:\n",
    "            chunks:  List of retrieved chunk texts.\n",
    "            query:   The user's question.\n",
    "\n",
    "        Returns:\n",
    "            List of compressed texts (empty/irrelevant chunks removed).\n",
    "        \"\"\"\n",
    "        compressed = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            result = self.compress(chunk, query)\n",
    "\n",
    "            if result:\n",
    "                compressed.append(result)\n",
    "\n",
    "        return compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12ce2ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualCompressorRetriever:\n",
    "    \"\"\"\n",
    "    Retriever that compresses each retrieved chunk to only the relevant parts.\n",
    "\n",
    "    Pipeline:\n",
    "        1. Index document with standard chunking → FAISS\n",
    "        2. On query: vector search → top-k chunks\n",
    "        3. For each chunk: LLM extracts only relevant portions\n",
    "        4. Filter out chunks where nothing was relevant\n",
    "        5. Return compressed extracts\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        embedding_model:   OpenAI embedding model.\n",
    "        compressor_model:  OpenAI model for chunk compression.\n",
    "        chunk_size:        Characters per chunk.\n",
    "        chunk_overlap:     Overlap between chunks.\n",
    "        k:                 Number of chunks to retrieve before compression.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model:str=\"text-embedding-3-small\",\n",
    "        compressor_model:str=\"gpt-4o-mini\",\n",
    "        chunk_size:int=1000,\n",
    "        chunk_overlap:int=200,\n",
    "        k:int=5\n",
    "        ):\n",
    "\n",
    "        self.chunk_size=chunk_size\n",
    "        self.chunk_overlap=chunk_overlap\n",
    "        self.k=k\n",
    "\n",
    "        self.embedder = OpenAIEmbedder(model=embedding_model)\n",
    "        self.vector_store = FAISSVectorStore(dimension=self.embedder.dimension)\n",
    "        self.compressor = ContextualCompressor(model_name=compressor_model)\n",
    "\n",
    "    \n",
    "    def index_document(self, text:str, doc_id:str=\"doc_0\")->int:\n",
    "        \"\"\"\n",
    "        Chunk and index a document. Standard RAG indexing — nothing special here.\n",
    "\n",
    "        Args:\n",
    "            text:    Full document text.\n",
    "            doc_id:  Document identifier.\n",
    "\n",
    "        Returns:\n",
    "            Number of chunks created.\n",
    "        \"\"\"\n",
    "        chunks = chunk_text(\n",
    "            text,\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap\n",
    "        )\n",
    "\n",
    "        documents = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            documents.append(\n",
    "                Document(\n",
    "                    content=chunk,\n",
    "                    metadata={\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"chunk_index\": i,\n",
    "                        \"total_chunks\": len(chunks),\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "\n",
    "        documents = self.embedder.embed_documents(documents)\n",
    "        self.vector_store.add_documents(documents)\n",
    "\n",
    "        return len(chunks)\n",
    "\n",
    "\n",
    "    def index_pdf(self, file_path: str, doc_id: Optional[str] = None) -> int:\n",
    "        \"\"\"Read and index a PDF file.\"\"\"\n",
    "        if doc_id is None:\n",
    "            doc_id = os.path.basename(file_path)\n",
    "        text = read_pdf(file_path)\n",
    "        return self.index_document(text, doc_id)\n",
    "\n",
    "\n",
    "\n",
    "    def index_text_file(self, file_path: str, doc_id: Optional[str] = None) -> int:\n",
    "        \"\"\"Read and index a text file.\"\"\"\n",
    "        if doc_id is None:\n",
    "            doc_id = os.path.basename(file_path)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        return self.index_document(text, doc_id)\n",
    "\n",
    "\n",
    "\n",
    "    def retrieve_raw(self, query: str) -> List[RetrievalResult]:\n",
    "        \"\"\"\n",
    "        Standard vector search (no compression). For comparison.\n",
    "\n",
    "        Args:\n",
    "            query:  Search query.\n",
    "\n",
    "        Returns:\n",
    "            List of RetrievalResult objects (full chunks).\n",
    "        \"\"\"\n",
    "        query_emb = self.embedder.embed_text(query)\n",
    "        return self.vector_store.search(query_emb, k=self.k)\n",
    "\n",
    "\n",
    "    \n",
    "    def retrieve_compressed(self, query:str)->Tuple[List[str],List[str]]:\n",
    "        \"\"\"\n",
    "        Retrieve chunks, then compress each to only the relevant parts.\n",
    "\n",
    "        This is the core method. The flow is:\n",
    "            1. Vector search → top-k full chunks\n",
    "            2. For each chunk → LLM extracts relevant portions\n",
    "            3. Filter out empty results\n",
    "\n",
    "        Args:\n",
    "            query:  Search query.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (compressed_texts, original_texts).\n",
    "            compressed_texts has irrelevant chunks filtered out.\n",
    "        \"\"\"\n",
    "\n",
    "        results = self.retrieve_raw(query)\n",
    "        original_texts = [r.document.content for r in results]\n",
    "\n",
    "        if not original_texts:\n",
    "            return [], []\n",
    "\n",
    "        \n",
    "        ## Now we compress each chunk\n",
    "\n",
    "        compressed_texts = self.compressor.compress_document(original_texts, query)\n",
    "\n",
    "        return compressed_texts, original_texts\n",
    "\n",
    "    def retrieve_context(self, query:str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Convenience method: return just the compressed texts.\n",
    "\n",
    "        Args:\n",
    "            query:  Search query.\n",
    "\n",
    "        Returns:\n",
    "            List of compressed context strings.\n",
    "        \"\"\"\n",
    "        compressed, _ = self.retrieve_compressed(query)\n",
    "        return compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24152e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualCompressionRAG:\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline with contextual compression.\n",
    "\n",
    "\n",
    "    Usage:\n",
    "        rag = ContextualCompressionRAG(file_path=\"report.pdf\")\n",
    "        answer, compressed = rag.query(\"What is the main topic?\")\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_path: str,\n",
    "        chunk_size: int = 1000,\n",
    "        chunk_overlap: int = 200,\n",
    "        k: int = 5,\n",
    "        embedding_model: str = \"text-embedding-3-small\",\n",
    "        compressor_model: str = \"gpt-4o-mini\",\n",
    "        chat_model: str = \"gpt-4o-mini\",\n",
    "        temperature: float = 0.0,\n",
    "        ):\n",
    "\n",
    "        \"\"\"\n",
    "        Initialize the Contextual Compression RAG pipeline.\n",
    "\n",
    "        Args:\n",
    "            file_path:         Path to document (PDF or text file).\n",
    "            chunk_size:        Characters per chunk.\n",
    "            chunk_overlap:     Overlap between chunks.\n",
    "            k:                 Number of chunks to retrieve before compression.\n",
    "            embedding_model:   OpenAI embedding model.\n",
    "            compressor_model:  OpenAI model for compressing chunks (fast/cheap).\n",
    "            chat_model:        OpenAI model for final answer generation.\n",
    "            temperature:       LLM temperature for answer generation.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "\n",
    "        self.retriever = ContextualCompressorRetriever(\n",
    "            embedding_model=embedding_model,\n",
    "            compressor_model=compressor_model,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            k=k\n",
    "        )\n",
    "\n",
    "        self.chat = OpenAIChat(\n",
    "            model_name=chat_model,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "\n",
    "        # Index the document\n",
    "        if file_path.endswith(\".pdf\"):\n",
    "            num_chunks = self.retriever.index_pdf(file_path)\n",
    "        else:\n",
    "            num_chunks = self.retriever.index_text_file(file_path)\n",
    "\n",
    "\n",
    "    def query(\n",
    "        self,\n",
    "        question: str,\n",
    "        return_context: bool = True,\n",
    "        ) -> Tuple[str, List[str]]:\n",
    "        \n",
    "        \"\"\"\n",
    "        Query the RAG system with contextual compression.\n",
    "\n",
    "        Flow:\n",
    "            1. Vector search → top-k full chunks\n",
    "            2. LLM compresses each chunk to relevant parts only\n",
    "            3. Compressed extracts → answer-generation LLM\n",
    "            4. Return answer + compressed contexts\n",
    "\n",
    "        Args:\n",
    "            question:        User's question.\n",
    "            return_context:  Whether to return compressed contexts.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (answer_string, list_of_compressed_context_strings).\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        compressed_contexts = self.retriever.retrieve_context(question)\n",
    "        \n",
    "        if not compressed_contexts:\n",
    "            return \"No Relevant Information found in the document.\", []\n",
    "        \n",
    "        answer = self.chat.chat_with_context(question, compressed_contexts)\n",
    "\n",
    "        if return_context:\n",
    "            return answer, compressed_contexts\n",
    "\n",
    "        return answer, []\n",
    "\n",
    "    \n",
    "    def compare(self, question:str)->None:\n",
    "        \"\"\"\n",
    "        Debug helper: compare raw chunks vs compressed extracts side by side.\n",
    "\n",
    "        Shows how much irrelevant content the compressor removes.\n",
    "\n",
    "        Args:\n",
    "            question:  Search query.\n",
    "        \"\"\"\n",
    "        compressed, originals = self.retriever.retrieve_compressed(question)\n",
    "\n",
    "        total_raw_chars = 0\n",
    "        for i, chunk in enumerate(originals):\n",
    "            total_raw_chars += len(chunk)\n",
    "            print(f\"\\n  Chunk {i + 1} ({len(chunk)} chars):\")\n",
    "            preview = chunk[:200].replace('\\n', ' ')\n",
    "            print(f\"    {preview}...\")\n",
    "\n",
    "\n",
    "        total_compressed_chars = 0\n",
    "        for i, extract in enumerate(compressed):\n",
    "            total_compressed_chars += len(extract)\n",
    "            print(f\"\\n  Extract {i + 1} ({len(extract)} chars):\")\n",
    "            preview = extract[:300].replace('\\n', ' ')\n",
    "            print(f\"    {preview}...\")\n",
    "\n",
    "        if total_raw_chars > 0:\n",
    "\n",
    "            ratio = (1 - total_compressed_chars / total_raw_chars) * 100\n",
    "            print(f\"    Raw:        {total_raw_chars:,} chars across {len(originals)} chunks\")\n",
    "            print(f\"    Compressed: {total_compressed_chars:,} chars across {len(compressed)} extracts\")\n",
    "            print(f\"    Reduction:  {ratio:.1f}%\")\n",
    "            print(f\"    Chunks filtered out: {len(originals) - len(compressed)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "110d066a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      "The main topic of the document is climate change and the various strategies and technologies, such as Carbon Capture and Storage (CCS), Carbon Utilization, and Direct Air Capture (DAC), that are being developed to mitigate its effects. It also discusses international agreements aimed at addressing climate change, the role of digital technologies, and the importance of research, education, and advocacy in combating climate change.\n",
      "\n",
      "Compressed Extracts:\n",
      "\n",
      "Extract 1:\n",
      "Carbon Capture and Storage (CCS) CCS technology captures CO2 emissions from industrial processes and power plants and stores it underground. This technology is critical for reducing emissions from hard-to-abate sectors. Developing cost-effective and scalable CCS solutions is essential for achieving climate goals. \n",
      "\n",
      "Carbon Utilization \n",
      "Utilizing captured CO2 to produce valuable products, such as synthetic fuels, chemicals, and building materials, can create economic opportunities and reduce emissions. Research into carbon utilization technologies is advancing, with potential applications in various industries. \n",
      "\n",
      "Direct Air Capture (DAC) \n",
      "DAC technology removes CO2 directly from the atmosphere, offering a way to achieve negative emissions. The captured CO2 can be stored or used in various applications. Scaling DAC technology and reducing costs are critical for its widespread adoption. \n",
      "\n",
      "Chapter 16: Global Cooperation and Governance \n",
      "International Agreements \n",
      "Paris Agreement\n",
      "\n",
      "Extract 2:\n",
      "Chapter 16: Global Cooperation and Governance \n",
      "International Agreements \n",
      "Paris Agreement The Paris Agreement is a landmark international accord that aims to limit global warming to \n",
      "well below 2 degrees Celsius above pre-industrial levels, with efforts to limit the increase to \n",
      "1.5 degrees Celsius. Countries submit nationally determined contributions (NDCs) outlining \n",
      "their climate action plans. Regular reviews and updates of NDCs are essential for meeting the \n",
      "agreement's goals. \n",
      "Kyoto Protocol \n",
      "The Kyoto Protocol, adopted in 1997, set binding emission reduction targets for developed \n",
      "countries. It was the first major international treaty to address climate change. The protocol \n",
      "laid the groundwork for subsequent agreements, highlighting the importance of collective \n",
      "action. \n",
      "Montreal Protocol \n",
      "The Montreal Protocol, designed to protect the ozone layer by phasing out ozone-depleting \n",
      "substances, has also contributed to climate mitigation. The Kigali Amendment to the protocol \n",
      "targets hydrofluorocarbons (HFCs), potent greenhouse gases, demonstrating the treaty's\n",
      "\n",
      "Extract 3:\n",
      "Most of these climate changes are attributed to very small variations in Earth's orbit that change the amount of solar energy our planet receives. During the Holocene epoch, which began at the end of the last ice age, human societies flourished, but the industrial era has seen unprecedented changes. Modern scientific observations indicate a rapid increase in global temperatures, sea levels, and extreme weather events. The Intergovernmental Panel on Climate Change (IPCC) has documented these changes extensively. The evidence overwhelmingly shows that recent changes are primarily driven by human activities, particularly the emission of greenhouse gases. The primary cause of recent climate change is the increase in greenhouse gases in the atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\"\n",
      "\n",
      "Extract 4:\n",
      "Digital Technologies  \n",
      "Smart Grids  \n",
      "Smart grids use digital technology to manage electricity distribution more efficiently. They enable real-time monitoring, demand response, and integration of distributed energy resources. Smart grids enhance grid reliability, reduce energy waste, and support the transition to renewable energy.  \n",
      "Internet of Things (IoT)  \n",
      "IoT devices can optimize energy use in homes, buildings, and industries. Smart thermostats, lighting systems, and appliances can adjust their operation based on occupancy and usage patterns, reducing energy consumption. IoT also enables predictive maintenance and efficient resource management.  \n",
      "Artificial Intelligence (AI)  \n",
      "AI can analyze large datasets to optimize energy systems, predict climate impacts, and enhance climate resilience. Applications include smart grid management, climate modeling, and precision agriculture. AI-driven solutions can improve efficiency, reduce costs, and support data-driven decision-making.  \n",
      "Carbon Capture and Utilization  \n",
      "Carbon Capture and Storage (CCS)\n",
      "\n",
      "Extract 5:\n",
      "Research and Innovation  \n",
      "Investing in research and innovation is essential for understanding and addressing the health impacts of climate change. This includes studying the links between climate and health, developing new technologies and treatments, and improving health data systems. Research informs evidence-based policies and interventions.  \n",
      "Chapter 11: Education and Advocacy  \n",
      "Climate Education  \n",
      "Curriculum Development  \n",
      "Integrating climate change into educational curricula is essential for raising awareness and building knowledge. Schools, colleges, and universities can incorporate climate science, sustainability, and environmental ethics into their programs. Educating the next generation fosters informed and engaged citizens.  \n",
      "Teacher Training  \n",
      "Providing training and resources for educators helps them effectively teach about climate change. Professional development programs, workshops, and online courses can enhance teachers' knowledge and skills. Supportive networks and communities of practice enable the sharing of ideas and experiences.\n"
     ]
    }
   ],
   "source": [
    "pdf_file_path = r\"C:\\Users\\TempAccess\\Documents\\Dhruv\\RAG\\data\\Understanding_Climate_Change.pdf\"\n",
    "\n",
    "rag = ContextualCompressionRAG(\n",
    "    file_path=pdf_file_path,\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    k=5,\n",
    "    compressor_model=\"gpt-4o-mini\",\n",
    "    chat_model=\"gpt-4o-mini\"\n",
    ")\n",
    "\n",
    "\n",
    "user_query = \"What is the main topic of the document?\"\n",
    "\n",
    "answer, compressed = rag.query(user_query)\n",
    "\n",
    "print(\"\\nAnswer:\")\n",
    "print(answer)\n",
    "\n",
    "print(\"\\nCompressed Extracts:\")\n",
    "for i, extract in enumerate(compressed):\n",
    "    print(f\"\\nExtract {i+1}:\")\n",
    "    print(extract)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31097711",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
