{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d46e45ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b760236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\TempAccess\\\\Documents\\\\Dhruv\\\\RAG\\\\advanced_retrieval'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "575184ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"c:\\\\Users\\\\TempAccess\\\\Documents\\\\Dhruv\\\\RAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "624251ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from helper_function_openai import (\n",
    "    Document,\n",
    "    RetrievalResult,\n",
    "    OpenAIEmbedder,\n",
    "    FAISSVectorStore,\n",
    "    OpenAIChat,\n",
    "    read_pdf_with_metadata,\n",
    "    chunk_text,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e422770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0472a536",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PageSummarizer:\n",
    "    \"\"\"\n",
    "    Summarizes document pages using OpenAI chat API.\n",
    "\n",
    "    Uses ThreadPoolExecutor for parallel summarization instead of asyncio,\n",
    "    which is simpler and doesn't require an event loop.\n",
    "\n",
    "    Args:\n",
    "        model_name:   OpenAI model for summarization.\n",
    "        temperature:  Should be 0 for consistent summaries.\n",
    "        max_workers:  Parallel summarization threads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"gpt-4o-mini\",\n",
    "        temperature: float = 0.0,\n",
    "        max_workers: int = 5,\n",
    "    ):\n",
    "        self.llm = OpenAIChat(\n",
    "            model_name=model_name,\n",
    "            temperature=temperature,\n",
    "            max_tokens=500,\n",
    "        )\n",
    "        self.max_workers = max_workers\n",
    "\n",
    "    def summarize_page(self, page_text: str) -> str:\n",
    "        \"\"\"\n",
    "        Summarize a single page/section.\n",
    "\n",
    "        Args:\n",
    "            page_text:  Full text of the page.\n",
    "\n",
    "        Returns:\n",
    "            Summary string (2-4 sentences).\n",
    "        \"\"\"\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a document summarizer. Write a concise 2-4 sentence \"\n",
    "                    \"summary of the provided text. Focus on the main topics and \"\n",
    "                    \"key information. Do not include preamble like 'This text discusses'.\"\n",
    "                ),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Summarize this text:\\n\\n{page_text}\",\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        return self.llm.chat(messages)\n",
    "\n",
    "    def summarize_pages(self, pages: List[Document]) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Summarize multiple pages in parallel using ThreadPoolExecutor.\n",
    "\n",
    "        Replaces the notebook's asyncio + batch + exponential backoff pattern\n",
    "        with a simpler threading approach.\n",
    "\n",
    "        Args:\n",
    "            pages:  List of Document objects (one per page).\n",
    "\n",
    "        Returns:\n",
    "            List of Document objects containing summaries with page metadata.\n",
    "        \"\"\"\n",
    "        summaries = [None] * len(pages)\n",
    "\n",
    "        def _summarize(idx: int, page: Document) -> Tuple[int, str]:\n",
    "            summary = self.summarize_page(page.content)\n",
    "            return idx, summary\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as pool:\n",
    "            futures = [\n",
    "                pool.submit(_summarize, i, page)\n",
    "                for i, page in enumerate(pages)\n",
    "            ]\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                idx, summary = future.result()\n",
    "                page = pages[idx]\n",
    "                summaries[idx] = Document(\n",
    "                    content=summary,\n",
    "                    metadata={\n",
    "                        \"source\": page.metadata.get(\"source\", \"\"),\n",
    "                        \"page\": page.metadata.get(\"page\", idx),\n",
    "                        \"is_summary\": True,\n",
    "                    },\n",
    "                )\n",
    "                print(f\"    Summarized page {page.metadata.get('page', idx)}\")\n",
    "\n",
    "        return summaries\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68ff420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalRetriever:\n",
    "    \"\"\"\n",
    "    Two-tier retriever: summaries (coarse) → detailed chunks (fine).\n",
    "\n",
    "    Architecture:\n",
    "        SUMMARY INDEX (FAISS):  One entry per page/section summary\n",
    "        DETAIL INDEX (FAISS):   Many entries per page (small chunks)\n",
    "        \n",
    "        Both share the same embedder. Each detail chunk stores its\n",
    "        page number in metadata for filtering.\n",
    "\n",
    "    Retrieval flow:\n",
    "        Query → search summaries → get relevant page numbers\n",
    "              → search detail chunks filtered by those pages\n",
    "              → return matched chunks\n",
    "\n",
    "    Args:\n",
    "        embedding_model:   OpenAI embedding model.\n",
    "        summary_model:     OpenAI model for page summarization.\n",
    "        chunk_size:        Characters per detail chunk.\n",
    "        chunk_overlap:     Overlap between detail chunks.\n",
    "        k_summaries:       Number of summary matches (pages to drill into).\n",
    "        k_chunks:          Number of detail chunks per matched page.\n",
    "        max_workers:       Parallel threads for summarization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model: str = \"text-embedding-3-small\",\n",
    "        summary_model: str = \"gpt-4o-mini\",\n",
    "        chunk_size: int = 1000,\n",
    "        chunk_overlap: int = 200,\n",
    "        k_summaries: int = 3,\n",
    "        k_chunks: int = 5,\n",
    "        max_workers: int = 5,\n",
    "    ):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.k_summaries = k_summaries\n",
    "        self.k_chunks = k_chunks\n",
    "\n",
    "        # Shared embedder for both indexes\n",
    "        self.embedder = OpenAIEmbedder(model=embedding_model)\n",
    "\n",
    "        # Two separate FAISS indexes\n",
    "        self.summary_store = FAISSVectorStore(dimension=self.embedder.dimension)\n",
    "        self.detail_store = FAISSVectorStore(dimension=self.embedder.dimension)\n",
    "\n",
    "        # Summarizer\n",
    "        self.summarizer = PageSummarizer(\n",
    "            model_name=summary_model,\n",
    "            max_workers=max_workers,\n",
    "        )\n",
    "\n",
    "        # Track page metadata for detail chunks (used for filtering)\n",
    "        self._detail_page_map: List[int] = []  # index → page number\n",
    "\n",
    "        # Stats\n",
    "        self.stats = {\n",
    "            \"pages\": 0,\n",
    "            \"summaries\": 0,\n",
    "            \"detail_chunks\": 0,\n",
    "        }\n",
    "\n",
    "    def index_pdf(self, file_path: str) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Index a PDF with hierarchical (summary + detail) indexing.\n",
    "\n",
    "        Full pipeline:\n",
    "            1. Read PDF → one Document per page\n",
    "            2. Summarize each page → store in summary index\n",
    "            3. Chunk each page → store in detail index (tagged with page #)\n",
    "\n",
    "        Args:\n",
    "            file_path:  Path to PDF file.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (num_summaries, num_detail_chunks).\n",
    "        \"\"\"\n",
    "        pages = read_pdf_with_metadata(file_path)\n",
    "        self.stats[\"pages\"] = len(pages)\n",
    "        print(f\"  [Hierarchical] {len(pages)} pages loaded\")\n",
    "\n",
    "        print(f\"  [Hierarchical] Summarizing {len(pages)} pages...\")\n",
    "        summary_docs = self.summarizer.summarize_pages(pages)\n",
    "        \n",
    "        summary_docs = self.embedder.embed_documents(summary_docs)\n",
    "        self.summary_store.add_documents(summary_docs)\n",
    "        self.stats[\"summaries\"] = len(summary_docs)\n",
    "        print(f\"  [Hierarchical] {len(summary_docs)} summaries indexed\")\n",
    "\n",
    "        detail_docs = []\n",
    "        for page in pages:\n",
    "            page_num = page.metadata.get(\"page\", 0)\n",
    "            chunks = chunk_text(\n",
    "                page.content,\n",
    "                chunk_size=self.chunk_size,\n",
    "                chunk_overlap=self.chunk_overlap,\n",
    "            )\n",
    "\n",
    "            for j, chunk_content in enumerate(chunks):\n",
    "                detail_docs.append(\n",
    "                    Document(\n",
    "                        content=chunk_content,\n",
    "                        metadata={\n",
    "                            \"source\": file_path,\n",
    "                            \"page\": page_num,\n",
    "                            \"chunk_index\": j,\n",
    "                            \"is_summary\": False,\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                self._detail_page_map.append(page_num)\n",
    "\n",
    "        detail_docs = self.embedder.embed_documents(detail_docs)\n",
    "        self.detail_store.add_documents(detail_docs)\n",
    "        self.stats[\"detail_chunks\"] = len(detail_docs)\n",
    "        print(f\"  [Hierarchical] {len(detail_docs)} detail chunks indexed\")\n",
    "\n",
    "        return len(summary_docs), len(detail_docs)\n",
    "\n",
    "    def index_text(self, text: str, doc_id: str = \"doc_0\") -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Index raw text by splitting into sections, summarizing, and chunking.\n",
    "\n",
    "        For non-PDF text, we split into large sections first (acting as \"pages\"),\n",
    "        then summarize and chunk each.\n",
    "\n",
    "        Args:\n",
    "            text:    Full document text.\n",
    "            doc_id:  Document identifier.\n",
    "\n",
    "        Returns:\n",
    "\n",
    "            Tuple of (num_summaries, num_detail_chunks).\n",
    "        \"\"\"\n",
    "        section_size = 3000\n",
    "        sections = chunk_text(text, chunk_size=section_size, chunk_overlap=200)\n",
    "\n",
    "        pages = []\n",
    "        for i, section in enumerate(sections):\n",
    "            pages.append(\n",
    "                Document(\n",
    "                    content=section,\n",
    "                    metadata={\"source\": doc_id, \"page\": i, \"total_pages\": len(sections)},\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.stats[\"pages\"] = len(pages)\n",
    "        print(f\"  [Hierarchical] {len(pages)} sections created\")\n",
    "\n",
    "        print(f\"  [Hierarchical] Summarizing {len(pages)} sections...\")\n",
    "        summary_docs = self.summarizer.summarize_pages(pages)\n",
    "        summary_docs = self.embedder.embed_documents(summary_docs)\n",
    "        self.summary_store.add_documents(summary_docs)\n",
    "        self.stats[\"summaries\"] = len(summary_docs)\n",
    "\n",
    "        detail_docs = []\n",
    "        for page in pages:\n",
    "            page_num = page.metadata.get(\"page\", 0)\n",
    "            chunks = chunk_text(\n",
    "                page.content,\n",
    "                chunk_size=self.chunk_size,\n",
    "                chunk_overlap=self.chunk_overlap,\n",
    "            )\n",
    "            for j, chunk_content in enumerate(chunks):\n",
    "                detail_docs.append(\n",
    "                    Document(\n",
    "                        content=chunk_content,\n",
    "                        metadata={\"source\": doc_id, \"page\": page_num, \"chunk_index\": j, \"is_summary\": False},\n",
    "                    )\n",
    "                )\n",
    "                self._detail_page_map.append(page_num)\n",
    "\n",
    "        detail_docs = self.embedder.embed_documents(detail_docs)\n",
    "        self.detail_store.add_documents(detail_docs)\n",
    "        self.stats[\"detail_chunks\"] = len(detail_docs)\n",
    "\n",
    "        return len(summary_docs), len(detail_docs)\n",
    "\n",
    "    def retrieve(self, query: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Hierarchical retrieval: summaries first, then detail chunks.\n",
    "\n",
    "        Flow:\n",
    "            1. Search SUMMARY index → top k_summaries pages\n",
    "            2. For each matched page, search DETAIL index\n",
    "               (manually filter by page number since FAISS doesn't\n",
    "               support metadata filtering natively)\n",
    "            3. Return all matched detail chunks\n",
    "\n",
    "        This replaces the notebook's retrieve_hierarchical() function\n",
    "        and its use of FAISS similarity_search(filter=page_filter).\n",
    "\n",
    "        Args:\n",
    "            query:  Search query.\n",
    "\n",
    "        Returns:\n",
    "            List of relevant detail chunk Documents.\n",
    "        \"\"\"\n",
    "        query_emb = self.embedder.embed_text(query)\n",
    "        summary_results = self.summary_store.search(query_emb, k=self.k_summaries)\n",
    "\n",
    "        relevant_pages = set()\n",
    "        for r in summary_results:\n",
    "            page = r.document.metadata.get(\"page\")\n",
    "            if page is not None:\n",
    "                relevant_pages.add(page)\n",
    "\n",
    "        if not relevant_pages:\n",
    "            return []\n",
    "\n",
    "        print(\n",
    "            f\"  [Hierarchical] Summary search → pages: {sorted(relevant_pages)}\"\n",
    "        )\n",
    "\n",
    "        search_k = min(\n",
    "            len(self.detail_store.documents),\n",
    "            self.k_chunks * len(relevant_pages) * 3,\n",
    "        )\n",
    "        detail_results = self.detail_store.search(query_emb, k=search_k)\n",
    "\n",
    "        filtered_chunks = []\n",
    "        page_counts = {p: 0 for p in relevant_pages}\n",
    "\n",
    "        for r in detail_results:\n",
    "            page = r.document.metadata.get(\"page\")\n",
    "            if page in relevant_pages and page_counts[page] < self.k_chunks:\n",
    "                filtered_chunks.append(r.document)\n",
    "                page_counts[page] += 1\n",
    "\n",
    "            if all(c >= self.k_chunks for c in page_counts.values()):\n",
    "                break\n",
    "\n",
    "        print(\n",
    "            f\"  [Hierarchical] Detail search → {len(filtered_chunks)} chunks \"\n",
    "            f\"from {len(relevant_pages)} pages\"\n",
    "        )\n",
    "\n",
    "        return filtered_chunks\n",
    "\n",
    "    def retrieve_context(self, query: str) -> List[str]:\n",
    "        \"\"\"Convenience: return just the chunk texts.\"\"\"\n",
    "        chunks = self.retrieve(query)\n",
    "        return [c.content for c in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a3809a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalIndicesRAG:\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline using hierarchical (summary + detail) indexing.\n",
    "\n",
    "    Combines HierarchicalRetriever with OpenAIChat for answer generation.\n",
    "\n",
    "    Usage:\n",
    "        rag = HierarchicalIndicesRAG(file_path=\"report.pdf\")\n",
    "        answer, chunks = rag.query(\"What is the greenhouse effect?\")\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_path: str,\n",
    "        chunk_size: int = 1000,\n",
    "        chunk_overlap: int = 200,\n",
    "        k_summaries: int = 3,\n",
    "        k_chunks: int = 5,\n",
    "        embedding_model: str = \"text-embedding-3-small\",\n",
    "        summary_model: str = \"gpt-4o-mini\",\n",
    "        chat_model: str = \"gpt-4o-mini\",\n",
    "        temperature: float = 0.0,\n",
    "        max_workers: int = 5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the Hierarchical Indices RAG pipeline.\n",
    "\n",
    "        Args:\n",
    "            file_path:         Path to document (PDF or text file).\n",
    "            chunk_size:        Characters per detail chunk.\n",
    "            chunk_overlap:     Overlap between detail chunks.\n",
    "            k_summaries:       Pages to match via summary search.\n",
    "            k_chunks:          Detail chunks per matched page.\n",
    "            embedding_model:   OpenAI embedding model.\n",
    "            summary_model:     OpenAI model for summarization.\n",
    "            chat_model:        OpenAI model for answer generation.\n",
    "            temperature:       LLM temperature for answers.\n",
    "            max_workers:       Parallel summarization threads.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "\n",
    "        self.retriever = HierarchicalRetriever(\n",
    "            embedding_model=embedding_model,\n",
    "            summary_model=summary_model,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            k_summaries=k_summaries,\n",
    "            k_chunks=k_chunks,\n",
    "            max_workers=max_workers,\n",
    "        )\n",
    "\n",
    "        self.chat = OpenAIChat(\n",
    "            model_name=chat_model,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"[Hierarchical] Indexing '{os.path.basename(file_path)}' \"\n",
    "            f\"(k_summaries={k_summaries}, k_chunks={k_chunks})...\"\n",
    "        )\n",
    "\n",
    "        if file_path.endswith(\".pdf\"):\n",
    "            n_sum, n_det = self.retriever.index_pdf(file_path)\n",
    "        else:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "            n_sum, n_det = self.retriever.index_text(text)\n",
    "\n",
    "        stats = self.retriever.stats\n",
    "        print(\n",
    "            f\"[Hierarchical] Done → {stats['pages']} pages, \"\n",
    "            f\"{n_sum} summaries, {n_det} detail chunks\"\n",
    "        )\n",
    "\n",
    "    def query(\n",
    "        self,\n",
    "        question: str,\n",
    "        return_context: bool = True,\n",
    "    ) -> Tuple[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Query the hierarchical RAG system.\n",
    "\n",
    "        Flow:\n",
    "            1. Search summaries → identify relevant pages\n",
    "            2. Search detail chunks within those pages\n",
    "            3. Feed detail chunks to answer LLM\n",
    "\n",
    "        Args:\n",
    "            question:        User's question.\n",
    "            return_context:  Whether to return retrieved chunks.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (answer_string, list_of_context_strings).\n",
    "        \"\"\"\n",
    "        contexts = self.retriever.retrieve_context(question)\n",
    "\n",
    "        if not contexts:\n",
    "            return \"No relevant information found in the document.\", []\n",
    "\n",
    "        answer = self.chat.chat_with_context(question, contexts)\n",
    "\n",
    "        if return_context:\n",
    "            return answer, contexts\n",
    "        return answer, []\n",
    "\n",
    "    def show_hierarchy(self, question: str) -> None:\n",
    "        \"\"\"\n",
    "        Debug helper: show the full hierarchical retrieval flow.\n",
    "\n",
    "        Displays which summaries matched, which pages were selected,\n",
    "        and which detail chunks were returned.\n",
    "\n",
    "        Args:\n",
    "            question:  Search query.\n",
    "        \"\"\"\n",
    "        print(f\"\\nQuery: {question}\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        # Step 1: Show summary matches\n",
    "        query_emb = self.retriever.embedder.embed_text(question)\n",
    "        summary_results = self.retriever.summary_store.search(\n",
    "            query_emb, k=self.retriever.k_summaries\n",
    "        )\n",
    "\n",
    "        print(f\"\\n TIER 1 — Summary Matches ({len(summary_results)}):\")\n",
    "        print(\"-\" * 50)\n",
    "        for i, r in enumerate(summary_results):\n",
    "            page = r.document.metadata.get(\"page\", \"?\")\n",
    "            print(f\"  {i+1}. [Page {page}, score={r.score:.4f}]\")\n",
    "            print(f\"     {r.document.content[:200]}...\")\n",
    "\n",
    "        # Step 2: Show detail matches\n",
    "        detail_chunks = self.retriever.retrieve(question)\n",
    "\n",
    "        print(f\"\\n TIER 2 — Detail Chunks ({len(detail_chunks)}):\")\n",
    "        print(\"-\" * 50)\n",
    "        for i, chunk in enumerate(detail_chunks):\n",
    "            page = chunk.metadata.get(\"page\", \"?\")\n",
    "            preview = chunk.content[:200].replace('\\n', ' ')\n",
    "            print(f\"  {i+1}. [Page {page}] {preview}...\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a34197d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hierarchical] Indexing 'Understanding_Climate_Change.pdf' (k_summaries=3, k_chunks=5)...\n",
      "  [Hierarchical] 33 pages loaded\n",
      "  [Hierarchical] Summarizing 33 pages...\n",
      "    Summarized page 3\n",
      "    Summarized page 5\n",
      "    Summarized page 1\n",
      "    Summarized page 2\n",
      "    Summarized page 4\n",
      "    Summarized page 6\n",
      "    Summarized page 7\n",
      "    Summarized page 9\n",
      "    Summarized page 8\n",
      "    Summarized page 10\n",
      "    Summarized page 12\n",
      "    Summarized page 11\n",
      "    Summarized page 14\n",
      "    Summarized page 15\n",
      "    Summarized page 13\n",
      "    Summarized page 16\n",
      "    Summarized page 17\n",
      "    Summarized page 18\n",
      "    Summarized page 19\n",
      "    Summarized page 20\n",
      "    Summarized page 21\n",
      "    Summarized page 23\n",
      "    Summarized page 22\n",
      "    Summarized page 24\n",
      "    Summarized page 27\n",
      "    Summarized page 25\n",
      "    Summarized page 26\n",
      "    Summarized page 28\n",
      "    Summarized page 29\n",
      "    Summarized page 33\n",
      "    Summarized page 31\n",
      "    Summarized page 32\n",
      "    Summarized page 30\n",
      "  [Hierarchical] 33 summaries indexed\n",
      "  [Hierarchical] 97 detail chunks indexed\n",
      "[Hierarchical] Done → 33 pages, 33 summaries, 97 detail chunks\n"
     ]
    }
   ],
   "source": [
    "pdf_file_path = r\"data\\Understanding_Climate_Change.pdf\"\n",
    "\n",
    "rag = HierarchicalIndicesRAG(\n",
    "        file_path=pdf_file_path,\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        k_summaries=3,\n",
    "        k_chunks=5,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21eca818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [Hierarchical] Summary search → pages: [1, 4, 18]\n",
      "  [Hierarchical] Detail search → 5 chunks from 3 pages\n",
      "\n",
      "Answer: Climate change refers to significant, long-term changes in the global climate, which includes the planet's overall weather patterns such as temperature, precipitation, and wind patterns over an extended period. It has been significantly influenced by human activities, particularly the burning of fossil fuels and deforestation.\n",
      "Detail chunks used: 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = input(\"User: \").strip()\n",
    "\n",
    "answer, context = rag.query(question)\n",
    "print(f\"\\nAnswer: {answer}\")\n",
    "print(f\"Detail chunks used: {len(context)}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e0cda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
