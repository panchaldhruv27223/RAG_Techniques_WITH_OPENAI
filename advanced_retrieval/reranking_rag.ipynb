{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ecea253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional, Tuple, Literal\n",
    "from dataclasses import dataclass\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "611ca27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a6c136e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\TempAccess\\\\Documents\\\\Dhruv\\\\RAG\\\\advanced_retrieval'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08d0826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"c:\\\\Users\\\\TempAccess\\\\Documents\\\\Dhruv\\\\RAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8841238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_function_openai import (\n",
    "    Document,\n",
    "    RetrievalResult,\n",
    "    OpenAIEmbedder,\n",
    "    FAISSVectorStore,\n",
    "    OpenAIChat,\n",
    "    read_pdf,\n",
    "    chunk_text,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5531b1b",
   "metadata": {},
   "source": [
    "# Method 1: LLM-Based Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63800299",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMReranker:\n",
    "    \"\"\"\n",
    "    Reranks chunks by asking an LLM to score each chunk's relevance (1-10)\n",
    "    to the query.\n",
    "\n",
    "    Args:\n",
    "        model_name:   OpenAI model for scoring.\n",
    "        temperature:  Should be 0 for consistent scoring.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"gpt-4o-mini\",\n",
    "        temperature: float = 0.0,\n",
    "    ):\n",
    "        self.llm = OpenAIChat(\n",
    "            model_name=model_name,\n",
    "            temperature=temperature,\n",
    "            max_tokens=100,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def score_chunk(self, query: str, chunk_text: str) -> float:\n",
    "        \"\"\"\n",
    "        Ask the LLM to rate a chunk's relevance to the query on a 1-10 scale.\n",
    "\n",
    "        Args:\n",
    "            query:       User's question.\n",
    "            chunk_text:  The chunk to score.\n",
    "\n",
    "        Returns:\n",
    "            Relevance score (1.0 to 10.0).\n",
    "        \"\"\"\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are a relevance scoring system. Rate the relevance of a \"\n",
    "                    \"document to a query on a scale of 1-10. Consider the specific \"\n",
    "                    \"context and intent of the query, not just keyword matches. \"\n",
    "                    \"Return JSON with key 'relevance_score' containing a number.\"\n",
    "                ),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    f\"Query: {query}\\n\\n\"\n",
    "                    f\"Document: {chunk_text}\\n\\n\"\n",
    "                    f\"Rate relevance (1-10):\"\n",
    "                ),\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "\n",
    "            result = self.llm.chat_json(messages)\n",
    "            score = float(result.get(\"relevance_score\", 0))\n",
    "            return max(1.0, min(10.0, score))\n",
    "\n",
    "\n",
    "        except (json.JSONDecodeError, ValueError, TypeError):\n",
    "\n",
    "            try:\n",
    "                raw = self.llm.chat(messages)\n",
    "                import re\n",
    "                numbers = re.findall(r'\\d+\\.?\\d*', raw)\n",
    "                if numbers:\n",
    "                    return max(1.0, min(10.0, float(numbers[0])))\n",
    "            except Exception:\n",
    "                pass\n",
    "            return 5.0 \n",
    "\n",
    "    def rerank(\n",
    "        self,\n",
    "        query: str,\n",
    "        chunks: List[str],\n",
    "        top_n: int = 3,\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Score and rerank all chunks, return top_n.\n",
    "\n",
    "        Args:\n",
    "            query:   User's question.\n",
    "            chunks:  List of chunk texts to rerank.\n",
    "            top_n:   Number of top chunks to return.\n",
    "\n",
    "        Returns:\n",
    "            List of (chunk_text, score) tuples, sorted by score descending.\n",
    "        \"\"\"\n",
    "        scored = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            score = self.score_chunk(query, chunk)\n",
    "            scored.append((chunk, score))\n",
    "            print(f\"    Chunk {i+1}/{len(chunks)}: score={score:.1f}\")\n",
    "\n",
    "        # Sort by score descending\n",
    "        scored.sort(key=lambda x: x[1], reverse=True)\n",
    "        return scored[:top_n]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6b83f5",
   "metadata": {},
   "source": [
    "# Method 2: Cross-Encoder Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecb33303",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEncoderReranker:\n",
    "    \"\"\"\n",
    "    Reranks chunks using a local cross-encoder model.\n",
    "\n",
    "    A cross-encoder takes a (query, document) pair as input and outputs\n",
    "    a single relevance score. Unlike bi-encoders (which embed query and\n",
    "    document separately), cross-encoders see both together ‚Äî enabling\n",
    "    deeper understanding of their relationship.\n",
    "\n",
    "    The default model (ms-marco-MiniLM-L-6-v2) is small (~80MB),\n",
    "    fast, and specifically trained for passage ranking.\n",
    "\n",
    "    Args:\n",
    "        model_name:  HuggingFace cross-encoder model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "    ):\n",
    "        try:\n",
    "            from sentence_transformers import CrossEncoder\n",
    "            self.model = CrossEncoder(model_name)\n",
    "            self._available = True\n",
    "        except ImportError:\n",
    "            print(\n",
    "                \"[CrossEncoder] sentence-transformers not installed. \"\n",
    "                \"Install with: pip install sentence-transformers\"\n",
    "            )\n",
    "            self.model = None\n",
    "            self._available = False\n",
    "\n",
    "    @property\n",
    "    def is_available(self) -> bool:\n",
    "        return self._available\n",
    "\n",
    "    def rerank(\n",
    "        self,\n",
    "        query: str,\n",
    "        chunks: List[str],\n",
    "        top_n: int = 3,\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Score and rerank chunks using the cross-encoder.\n",
    "\n",
    "        All (query, chunk) pairs are scored in a single batch call ‚Äî\n",
    "        much faster than N separate LLM calls.\n",
    "\n",
    "        Args:\n",
    "            query:   User's question.\n",
    "            chunks:  List of chunk texts to rerank.\n",
    "            top_n:   Number of top chunks to return.\n",
    "\n",
    "        Returns:\n",
    "            List of (chunk_text, score) tuples, sorted by score descending.\n",
    "        \"\"\"\n",
    "        if not self._available:\n",
    "            raise RuntimeError(\"Cross-encoder not available. Install sentence-transformers.\")\n",
    "\n",
    "        # Create query-document pairs\n",
    "        pairs = [[query, chunk] for chunk in chunks]\n",
    "\n",
    "        # Score all pairs in one batch\n",
    "        scores = self.model.predict(pairs)\n",
    "\n",
    "        # Pair chunks with scores and sort\n",
    "        scored = list(zip(chunks, [float(s) for s in scores]))\n",
    "        scored.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return scored[:top_n]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd46790a",
   "metadata": {},
   "source": [
    "# Reranking Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4153633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RerankingRetriever:\n",
    "    \"\"\"\n",
    "    Retriever that performs initial vector search then reranks results.\n",
    "\n",
    "    Pipeline:\n",
    "        1. Index document ‚Üí FAISS\n",
    "        2. On query: vector search ‚Üí top initial_k candidates\n",
    "        3. Rerank candidates using LLM or cross-encoder\n",
    "        4. Return top final_k reranked results\n",
    "\n",
    "    Args:\n",
    "        embedding_model:  OpenAI embedding model.\n",
    "        rerank_method:    \"llm\" or \"cross_encoder\".\n",
    "        reranker_model:   Model name for the reranker.\n",
    "        chunk_size:       Characters per chunk.\n",
    "        chunk_overlap:    Overlap between chunks.\n",
    "        initial_k:        Number of candidates from vector search.\n",
    "        final_k:          Number of results after reranking.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model: str = \"text-embedding-3-small\",\n",
    "        rerank_method: Literal[\"llm\", \"cross_encoder\"] = \"llm\",\n",
    "        reranker_model: Optional[str] = None,\n",
    "        chunk_size: int = 1000,\n",
    "        chunk_overlap: int = 200,\n",
    "        initial_k: int = 15,\n",
    "        final_k: int = 3,\n",
    "    ):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.initial_k = initial_k\n",
    "        self.final_k = final_k\n",
    "        self.rerank_method = rerank_method\n",
    "\n",
    "        # Core components\n",
    "        self.embedder = OpenAIEmbedder(model=embedding_model)\n",
    "        self.vector_store = FAISSVectorStore(dimension=self.embedder.dimension)\n",
    "\n",
    "        # Initialize reranker based on method\n",
    "        if rerank_method == \"llm\":\n",
    "            model = reranker_model or \"gpt-4o-mini\"\n",
    "            self.reranker = LLMReranker(model_name=model)\n",
    "        elif rerank_method == \"cross_encoder\":\n",
    "            model = reranker_model or \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "            self.reranker = CrossEncoderReranker(model_name=model)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown rerank method: {rerank_method}\")\n",
    "\n",
    "    def index_document(self, text: str, doc_id: str = \"doc_0\") -> int:\n",
    "        \"\"\"Chunk and index a document in FAISS.\"\"\"\n",
    "        chunks = chunk_text(\n",
    "            text,\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap,\n",
    "        )\n",
    "\n",
    "        documents = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            documents.append(\n",
    "                Document(\n",
    "                    content=chunk,\n",
    "                    metadata={\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"chunk_index\": i,\n",
    "                        \"total_chunks\": len(chunks),\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "\n",
    "        documents = self.embedder.embed_documents(documents)\n",
    "        self.vector_store.add_documents(documents)\n",
    "        return len(chunks)\n",
    "\n",
    "    def index_pdf(self, file_path: str, doc_id: Optional[str] = None) -> int:\n",
    "        if doc_id is None:\n",
    "            doc_id = os.path.basename(file_path)\n",
    "        text = read_pdf(file_path)\n",
    "        return self.index_document(text, doc_id)\n",
    "\n",
    "    def index_text_file(self, file_path: str, doc_id: Optional[str] = None) -> int:\n",
    "        if doc_id is None:\n",
    "            doc_id = os.path.basename(file_path)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        return self.index_document(text, doc_id)\n",
    "\n",
    "    def retrieve_without_rerank(self, query: str, k: Optional[int] = None) -> List[str]:\n",
    "        \"\"\"\n",
    "        Standard vector search (no reranking). For comparison.\n",
    "\n",
    "        Args:\n",
    "            query:  Search query.\n",
    "            k:      Number of results (defaults to final_k).\n",
    "\n",
    "        Returns:\n",
    "            List of chunk texts.\n",
    "        \"\"\"\n",
    "        if k is None:\n",
    "            k = self.final_k\n",
    "        query_emb = self.embedder.embed_text(query)\n",
    "        results = self.vector_store.search(query_emb, k=k)\n",
    "        return [r.document.content for r in results]\n",
    "\n",
    "    def retrieve(self, query: str) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Retrieve with reranking: vector search ‚Üí rerank ‚Üí top results.\n",
    "\n",
    "        Args:\n",
    "            query:  Search query.\n",
    "\n",
    "        Returns:\n",
    "            List of (chunk_text, rerank_score) tuples.\n",
    "        \"\"\"\n",
    "        # Step 1: Initial broad vector search\n",
    "        query_emb = self.embedder.embed_text(query)\n",
    "        results = self.vector_store.search(query_emb, k=self.initial_k)\n",
    "        candidate_chunks = [r.document.content for r in results]\n",
    "\n",
    "        if not candidate_chunks:\n",
    "            return []\n",
    "\n",
    "        # Step 2: Rerank\n",
    "        print(f\"  [Reranker] Reranking {len(candidate_chunks)} candidates ({self.rerank_method})...\")\n",
    "        reranked = self.reranker.rerank(query, candidate_chunks, top_n=self.final_k)\n",
    "        print(f\"  [Reranker] Top {len(reranked)} selected\")\n",
    "\n",
    "        return reranked\n",
    "\n",
    "    def retrieve_context(self, query: str) -> List[str]:\n",
    "        \"\"\"Convenience: return just the reranked text strings.\"\"\"\n",
    "        reranked = self.retrieve(query)\n",
    "        return [text for text, _ in reranked]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4aef678",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RerankingRAG:\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline with reranking.\n",
    "\n",
    "    Combines RerankingRetriever (vector search + reranking) with\n",
    "    OpenAIChat (for answer generation).\n",
    "\n",
    "    Replaces LangChain's:\n",
    "        - CustomRetriever(BaseRetriever)\n",
    "        - CrossEncoderRetriever(BaseRetriever)\n",
    "        - RetrievalQA.from_chain_type()\n",
    "\n",
    "    Usage:\n",
    "        rag = RerankingRAG(file_path=\"report.pdf\", rerank_method=\"llm\")\n",
    "        answer, contexts = rag.query(\"What are the impacts on biodiversity?\")\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_path: str,\n",
    "        rerank_method: Literal[\"llm\", \"cross_encoder\"] = \"llm\",\n",
    "        reranker_model: Optional[str] = None,\n",
    "        chunk_size: int = 1000,\n",
    "        chunk_overlap: int = 200,\n",
    "        final_k: int = 3,\n",
    "        embedding_model: str = \"text-embedding-3-small\",\n",
    "        chat_model: str = \"gpt-4o-mini\",\n",
    "        temperature: float = 0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the Reranking RAG pipeline.\n",
    "\n",
    "        Args:\n",
    "            file_path:        Path to document (PDF or text file).\n",
    "            rerank_method:    \"llm\" (accurate, slow) or \"cross_encoder\" (fast, local).\n",
    "            reranker_model:   Model name override for reranker.\n",
    "            chunk_size:       Characters per chunk.\n",
    "            chunk_overlap:    Overlap between chunks.\n",
    "            initial_k:        Candidates from vector search (cast wide net).\n",
    "            final_k:          Results after reranking (keep the best).\n",
    "            embedding_model:  OpenAI embedding model.\n",
    "            chat_model:       OpenAI model for answer generation.\n",
    "            temperature:      LLM temperature for answers.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.initial_k=final_k*5\n",
    "        # Initialize retriever with reranking\n",
    "        self.retriever = RerankingRetriever(\n",
    "            embedding_model=embedding_model,\n",
    "            rerank_method=rerank_method,\n",
    "            reranker_model=reranker_model,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            initial_k=self.initial_k,\n",
    "            final_k=final_k,\n",
    "        )\n",
    "\n",
    "        # Initialize chat model\n",
    "        self.chat = OpenAIChat(\n",
    "            model_name=chat_model,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "        # Index the document\n",
    "        if file_path.endswith(\".pdf\"):\n",
    "            num_chunks = self.retriever.index_pdf(file_path)\n",
    "        else:\n",
    "            num_chunks = self.retriever.index_text_file(file_path)\n",
    "\n",
    "        print(\n",
    "            f\"[Reranking] Indexed '{os.path.basename(file_path)}' ‚Üí {num_chunks} chunks \"\n",
    "            f\"(method={rerank_method}, initial_k={self.initial_k}, final_k={final_k})\"\n",
    "        )\n",
    "\n",
    "    def query(\n",
    "        self,\n",
    "        question: str,\n",
    "        return_context: bool = True,\n",
    "    ) -> Tuple[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Query the RAG system with reranking.\n",
    "\n",
    "        Flow:\n",
    "            1. Vector search ‚Üí initial_k candidates (cast wide net)\n",
    "            2. Rerank ‚Üí final_k best chunks (filter the best)\n",
    "            3. Feed reranked chunks to answer LLM\n",
    "\n",
    "        Args:\n",
    "            question:        User's question.\n",
    "            return_context:  Whether to return reranked contexts.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (answer_string, list_of_context_strings).\n",
    "        \"\"\"\n",
    "        contexts = self.retriever.retrieve_context(question)\n",
    "\n",
    "        if not contexts:\n",
    "            return \"No relevant information found in the document.\", []\n",
    "\n",
    "        answer = self.chat.chat_with_context(question, contexts)\n",
    "\n",
    "        if return_context:\n",
    "            return answer, contexts\n",
    "        return answer, []\n",
    "\n",
    "    def compare(self, question: str) -> None:\n",
    "        \"\"\"\n",
    "        Debug helper: compare baseline vector search vs reranked results.\n",
    "\n",
    "        Shows how reranking reorders chunks ‚Äî the key \"capital of France\"\n",
    "        demonstration from the notebook.\n",
    "\n",
    "        Args:\n",
    "            question:  Search query.\n",
    "        \"\"\"\n",
    "        print(f\"\\nQuery: {question}\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        # Baseline: top final_k from vector search only\n",
    "        baseline = self.retriever.retrieve_without_rerank(question)\n",
    "        print(f\"\\nüì¶ BASELINE (vector search, top {len(baseline)}):\")\n",
    "        print(\"-\" * 50)\n",
    "        for i, ctx in enumerate(baseline):\n",
    "            preview = ctx[:200].replace('\\n', ' ')\n",
    "            print(f\"  {i+1}. {preview}...\")\n",
    "\n",
    "        # Reranked\n",
    "        reranked = self.retriever.retrieve(question)\n",
    "        print(f\"\\nüîç RERANKED ({self.retriever.rerank_method}, top {len(reranked)}):\")\n",
    "        print(\"-\" * 50)\n",
    "        for i, (ctx, score) in enumerate(reranked):\n",
    "            preview = ctx[:200].replace('\\n', ' ')\n",
    "            print(f\"  {i+1}. [score={score:.2f}] {preview}...\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a82d9d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:00<00:00, 304.58it/s, Materializing param=classifier.weight]                                    \n",
      "\u001b[1mBertForSequenceClassification LOAD REPORT\u001b[0m from: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Reranking] Indexed 'Understanding_Climate_Change.pdf' ‚Üí 76 chunks (method=cross_encoder, initial_k=15, final_k=3)\n",
      "  [Reranker] Reranking 15 candidates (cross_encoder)...\n",
      "  [Reranker] Top 3 selected\n",
      "\n",
      "Answer: The primary causes of climate change, as outlined in the context, are:\n",
      "\n",
      "1. **Increase in Greenhouse Gases**: The rise in greenhouse gases such as carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O) in the atmosphere, which trap heat from the sun and create a \"greenhouse effect.\"\n",
      "\n",
      "2. **Human Activities**: Significant contributions to climate change are attributed to human activities, particularly the burning of fossil fuels and deforestation.\n",
      "\n",
      "3. **Agricultural Practices**: Emissions from the agricultural sector, which can be mitigated through the development of eco-friendly fertilizers and farming techniques.\n",
      "Reranked chunks used: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf_path = r\"data\\Understanding_Climate_Change.pdf\"\n",
    "\n",
    "\n",
    "rag = RerankingRAG(\n",
    "    file_path=pdf_path,\n",
    "    rerank_method=\"cross_encoder\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    final_k=3,\n",
    ")\n",
    "\n",
    "# rag.compare(\"What are the impacts of climate change on biodiversity?\")\n",
    "\n",
    "question = input(\"User: \").strip()\n",
    "answer, context = rag.query(question)\n",
    "print(f\"\\nAnswer: {answer}\")\n",
    "print(f\"Reranked chunks used: {len(context)}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e44b1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f7f5d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32a9777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5621d3fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00005c55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5969d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566b4999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751c3fec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
