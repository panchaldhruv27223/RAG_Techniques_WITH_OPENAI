{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aae842e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ce147ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\TempAccess\\\\Documents\\\\Dhruv\\\\RAG\\\\advanced_retrieval'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33417e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"c:\\\\Users\\\\TempAccess\\\\Documents\\\\Dhruv\\\\RAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e4c7b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "from helper_function_openai import (\n",
    "    Document,\n",
    "    RetrievalResult,\n",
    "    OpenAIEmbedder,\n",
    "    FAISSVectorStore,\n",
    "    OpenAIChat,\n",
    "    read_pdf,\n",
    "    chunk_text,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a498af8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25Index:\n",
    "    \"\"\"\n",
    "    BM25 (Best Matching 25) keyword-based retrieval index.\n",
    "\n",
    "    BM25 is a probabilistic ranking function based on TF-IDF. It scores\n",
    "    documents by how frequently query terms appear, with diminishing\n",
    "    returns for repeated terms and length normalization.\n",
    "\n",
    "    Unlike vector search, BM25:\n",
    "        - Finds EXACT keyword matches (acronyms, names, codes)\n",
    "        - Doesn't understand paraphrasing or synonyms\n",
    "        - Is very fast (no embedding computation needed)\n",
    "        - Works well for specific/technical queries\n",
    "\n",
    "    Args:\n",
    "        documents:  List of Document objects to index.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._index: Optional[BM25Okapi] = None\n",
    "        self._documents: List[Document] = []\n",
    "\n",
    "    def build(self, documents: List[Document]) -> None:\n",
    "        \"\"\"\n",
    "        Build the BM25 index from documents.\n",
    "\n",
    "        Tokenization is simple whitespace splitting — could be improved\n",
    "        with stemming/lemmatization for production use.\n",
    "\n",
    "        Args:\n",
    "            documents:  List of Document objects.\n",
    "        \"\"\"\n",
    "        self._documents = documents\n",
    "\n",
    "        # Tokenize: split each document's content on whitespace\n",
    "        tokenized_docs = [doc.content.lower().split() for doc in documents]\n",
    "        self._index = BM25Okapi(tokenized_docs)\n",
    "\n",
    "    def score_all(self, query: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Score ALL documents against a query using BM25.\n",
    "\n",
    "        Returns raw BM25 scores for every document in the index.\n",
    "        Higher score = more keyword overlap with the query.\n",
    "\n",
    "        Args:\n",
    "            query:  Search query string.\n",
    "\n",
    "        Returns:\n",
    "            numpy array of BM25 scores, one per document.\n",
    "        \"\"\"\n",
    "        if self._index is None:\n",
    "            raise ValueError(\"BM25 index not built. Call build() first.\")\n",
    "\n",
    "        tokenized_query = query.lower().split()\n",
    "        return self._index.get_scores(tokenized_query)\n",
    "\n",
    "    @property\n",
    "    def documents(self) -> List[Document]:\n",
    "        return self._documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29ad2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionRetriever:\n",
    "    \"\"\"\n",
    "    Retriever that fuses vector-based (semantic) and BM25 (keyword) search.\n",
    "\n",
    "    Pipeline:\n",
    "        1. Index document → both FAISS vector store AND BM25 index\n",
    "        2. On query:\n",
    "           a. Vector search → score all chunks by semantic similarity\n",
    "           b. BM25 search → score all chunks by keyword relevance\n",
    "           c. Normalize both score sets to [0, 1]\n",
    "           d. Combine: alpha × vector + (1-alpha) × bm25\n",
    "           e. Rank by combined score → return top-k\n",
    "\n",
    "    The notebook retrieves ALL docs from the vectorstore to get scores\n",
    "    for every chunk. We do the same via FAISS's search with k=total.\n",
    "\n",
    "    Args:\n",
    "        embedding_model:  OpenAI embedding model.\n",
    "        chunk_size:       Characters per chunk.\n",
    "        chunk_overlap:    Overlap between chunks.\n",
    "        k:                Number of top results to return.\n",
    "        alpha:            Weight for vector scores (0=pure BM25, 1=pure vector).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model: str = \"text-embedding-3-small\",\n",
    "        chunk_size: int = 1000,\n",
    "        chunk_overlap: int = 200,\n",
    "        k: int = 5,\n",
    "        alpha: float = 0.5,\n",
    "    ):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Core components\n",
    "        self.embedder = OpenAIEmbedder(model=embedding_model)\n",
    "        self.vector_store = FAISSVectorStore(dimension=self.embedder.dimension)\n",
    "        self.bm25_index = BM25Index()\n",
    "\n",
    "        # Keep reference to all chunks for scoring\n",
    "        self._all_chunks: List[Document] = []\n",
    "\n",
    "    def index_document(self, text: str, doc_id: str = \"doc_0\") -> int:\n",
    "        \"\"\"\n",
    "        Chunk and index a document in BOTH vector store and BM25 index.\n",
    "\n",
    "        Args:\n",
    "            text:    Full document text.\n",
    "            doc_id:  Document identifier.\n",
    "\n",
    "        Returns:\n",
    "            Number of chunks created.\n",
    "        \"\"\"\n",
    "        # Clean text: replace tabs with spaces (matching notebook's replace_t_with_space)\n",
    "        text = text.replace('\\t', ' ')\n",
    "\n",
    "        # Chunk the text\n",
    "        chunks = chunk_text(\n",
    "            text,\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap,\n",
    "        )\n",
    "\n",
    "        # Create Document objects\n",
    "        documents = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            documents.append(\n",
    "                Document(\n",
    "                    content=chunk,\n",
    "                    metadata={\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"chunk_index\": i,\n",
    "                        \"total_chunks\": len(chunks),\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self._all_chunks = documents\n",
    "\n",
    "        # Index in FAISS (vector search)\n",
    "        embedded_docs = self.embedder.embed_documents(documents)\n",
    "        self.vector_store.add_documents(embedded_docs)\n",
    "\n",
    "        # Index in BM25 (keyword search)\n",
    "        self.bm25_index.build(documents)\n",
    "\n",
    "        return len(chunks)\n",
    "\n",
    "    def index_pdf(self, file_path: str, doc_id: Optional[str] = None) -> int:\n",
    "        \"\"\"Read and index a PDF file.\"\"\"\n",
    "        if doc_id is None:\n",
    "            doc_id = os.path.basename(file_path)\n",
    "        text = read_pdf(file_path)\n",
    "        return self.index_document(text, doc_id)\n",
    "\n",
    "    def index_text_file(self, file_path: str, doc_id: Optional[str] = None) -> int:\n",
    "        \"\"\"Read and index a text file.\"\"\"\n",
    "        if doc_id is None:\n",
    "            doc_id = os.path.basename(file_path)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        return self.index_document(text, doc_id)\n",
    "\n",
    "    def _normalize_scores(self, scores: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Normalize scores to [0, 1] range using min-max normalization.\n",
    "\n",
    "        Args:\n",
    "            scores:  Raw score array.\n",
    "\n",
    "        Returns:\n",
    "            Normalized scores in [0, 1].\n",
    "        \"\"\"\n",
    "        epsilon = 1e-8\n",
    "        min_s = np.min(scores)\n",
    "        max_s = np.max(scores)\n",
    "        return (scores - min_s) / (max_s - min_s + epsilon)\n",
    "\n",
    "    def retrieve(self, query: str, alpha: Optional[float] = None) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Perform fusion retrieval combining vector and BM25 search.\n",
    "\n",
    "        This is the core method. Both search methods score ALL chunks,\n",
    "        scores are normalized to [0,1], then combined with alpha weighting.\n",
    "\n",
    "        Args:\n",
    "            query:  Search query.\n",
    "            alpha:  Optional override for vector weight. If None, uses self.alpha.\n",
    "\n",
    "        Returns:\n",
    "            List of Document objects, ranked by combined score.\n",
    "        \"\"\"\n",
    "        if alpha is None:\n",
    "            alpha = self.alpha\n",
    "\n",
    "        total_docs = len(self._all_chunks)\n",
    "        if total_docs == 0:\n",
    "            return []\n",
    "\n",
    "        # Step 1: Vector search — score ALL chunks\n",
    "        query_emb = self.embedder.embed_text(query)\n",
    "        vector_results = self.vector_store.search(query_emb, k=total_docs)\n",
    "\n",
    "        # Build a map: chunk_index → vector score\n",
    "        # FAISS with IndexFlatIP returns inner product (higher = better)\n",
    "        vector_score_map = {}\n",
    "        for r in vector_results:\n",
    "            idx = r.document.metadata.get(\"chunk_index\", -1)\n",
    "            vector_score_map[idx] = r.score\n",
    "\n",
    "        # Create ordered vector scores array\n",
    "        vector_scores = np.array([\n",
    "            vector_score_map.get(i, 0.0) for i in range(total_docs)\n",
    "        ])\n",
    "\n",
    "        # Step 2: BM25 search — score ALL chunks\n",
    "        bm25_scores = self.bm25_index.score_all(query)\n",
    "\n",
    "        # Step 3: Normalize both to [0, 1]\n",
    "        vector_scores_norm = self._normalize_scores(vector_scores)\n",
    "        bm25_scores_norm = self._normalize_scores(bm25_scores)\n",
    "\n",
    "        # Step 4: Combine with alpha weighting\n",
    "        combined_scores = alpha * vector_scores_norm + (1 - alpha) * bm25_scores_norm\n",
    "\n",
    "        # Step 5: Rank and return top-k\n",
    "        sorted_indices = np.argsort(combined_scores)[::-1]\n",
    "        top_indices = sorted_indices[:self.k]\n",
    "\n",
    "        return [self._all_chunks[i] for i in top_indices]\n",
    "\n",
    "    def retrieve_context(self, query: str, alpha: Optional[float] = None) -> List[str]:\n",
    "        \"\"\"\n",
    "        Convenience method: return just the text strings.\n",
    "\n",
    "        Args:\n",
    "            query:  Search query.\n",
    "            alpha:  Optional vector weight override.\n",
    "\n",
    "        Returns:\n",
    "            List of chunk text strings.\n",
    "        \"\"\"\n",
    "        docs = self.retrieve(query, alpha)\n",
    "        return [doc.content for doc in docs]\n",
    "\n",
    "    def retrieve_with_scores(self, query: str, alpha: Optional[float] = None) -> List[Tuple[Document, float, float, float]]:\n",
    "        \"\"\"\n",
    "        Retrieve with detailed score breakdown for debugging.\n",
    "\n",
    "        Args:\n",
    "            query:  Search query.\n",
    "            alpha:  Optional vector weight override.\n",
    "\n",
    "        Returns:\n",
    "            List of (document, vector_score, bm25_score, combined_score) tuples.\n",
    "        \"\"\"\n",
    "        if alpha is None:\n",
    "            alpha = self.alpha\n",
    "\n",
    "        total_docs = len(self._all_chunks)\n",
    "        if total_docs == 0:\n",
    "            return []\n",
    "\n",
    "        # Vector scores\n",
    "        query_emb = self.embedder.embed_text(query)\n",
    "        vector_results = self.vector_store.search(query_emb, k=total_docs)\n",
    "        vector_score_map = {}\n",
    "        for r in vector_results:\n",
    "            idx = r.document.metadata.get(\"chunk_index\", -1)\n",
    "            vector_score_map[idx] = r.score\n",
    "        vector_scores = np.array([vector_score_map.get(i, 0.0) for i in range(total_docs)])\n",
    "\n",
    "        # BM25 scores\n",
    "        bm25_scores = self.bm25_index.score_all(query)\n",
    "\n",
    "        # Normalize\n",
    "        v_norm = self._normalize_scores(vector_scores)\n",
    "        b_norm = self._normalize_scores(bm25_scores)\n",
    "        combined = alpha * v_norm + (1 - alpha) * b_norm\n",
    "        # Sort and return top-k with score breakdown\n",
    "        sorted_indices = np.argsort(combined)[::-1][:self.k]\n",
    "\n",
    "        results = []\n",
    "        for i in sorted_indices:\n",
    "            results.append((\n",
    "                self._all_chunks[i],\n",
    "                float(v_norm[i]),\n",
    "                float(b_norm[i]),\n",
    "                float(combined[i]),\n",
    "            ))\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49c9142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionRetrievalRAG:\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline using fusion retrieval (vector + BM25).\n",
    "\n",
    "    Usage:\n",
    "        rag = FusionRetrievalRAG(file_path=\"report.pdf\", alpha=0.5)\n",
    "        answer, contexts = rag.query(\"What are the impacts of climate change?\")\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_path: str,\n",
    "        chunk_size: int = 1000,\n",
    "        chunk_overlap: int = 200,\n",
    "        k: int = 5,\n",
    "        alpha: float = 0.5,\n",
    "        embedding_model: str = \"text-embedding-3-small\",\n",
    "        chat_model: str = \"gpt-4o-mini\",\n",
    "        temperature: float = 0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the Fusion Retrieval RAG pipeline.\n",
    "\n",
    "        Args:\n",
    "            file_path:        Path to document (PDF or text file).\n",
    "            chunk_size:        Characters per chunk.\n",
    "            chunk_overlap:     Overlap between chunks.\n",
    "            k:                 Number of top results to return.\n",
    "            alpha:             Vector weight (0=pure BM25, 1=pure vector, 0.5=equal).\n",
    "            embedding_model:   OpenAI embedding model.\n",
    "            chat_model:        OpenAI chat model.\n",
    "            temperature:       LLM temperature.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "\n",
    "        # Initialize retriever\n",
    "        self.retriever = FusionRetriever(\n",
    "            embedding_model=embedding_model,\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            k=k,\n",
    "            alpha=alpha,\n",
    "        )\n",
    "\n",
    "        # Initialize chat model\n",
    "        self.chat = OpenAIChat(\n",
    "            model_name=chat_model,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "\n",
    "        # Index the document\n",
    "        if file_path.endswith(\".pdf\"):\n",
    "            num_chunks = self.retriever.index_pdf(file_path)\n",
    "        else:\n",
    "            num_chunks = self.retriever.index_text_file(file_path)\n",
    "\n",
    "        print(\n",
    "            f\"[FusionRetrieval] Indexed '{os.path.basename(file_path)}' \"\n",
    "            f\"→ {num_chunks} chunks (alpha={alpha}, k={k})\"\n",
    "        )\n",
    "        print(f\"[FusionRetrieval] Both FAISS (vector) and BM25 (keyword) indexes built\")\n",
    "\n",
    "    def query(\n",
    "        self,\n",
    "        question: str,\n",
    "        return_context: bool = True,\n",
    "        alpha: Optional[float] = None,\n",
    "    ) -> Tuple[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Query the fusion RAG system.\n",
    "\n",
    "        Args:\n",
    "            question:        User's question.\n",
    "            return_context:  Whether to return retrieved contexts.\n",
    "            alpha:           Optional override for vector/BM25 weight.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (answer_string, list_of_context_strings).\n",
    "        \"\"\"\n",
    "        contexts = self.retriever.retrieve_context(question, alpha)\n",
    "\n",
    "        if not contexts:\n",
    "            return \"No relevant information found in the document.\", []\n",
    "\n",
    "        answer = self.chat.chat_with_context(question, contexts)\n",
    "\n",
    "        if return_context:\n",
    "            return answer, contexts\n",
    "        return answer, []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a283c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FusionRetrieval] Indexed 'Understanding_Climate_Change.pdf' → 76 chunks (alpha=0.5, k=5)\n",
      "[FusionRetrieval] Both FAISS (vector) and BM25 (keyword) indexes built\n"
     ]
    }
   ],
   "source": [
    "pdf_path = r\"data\\Understanding_Climate_Change.pdf\"\n",
    "\n",
    "rag = FusionRetrievalRAG(\n",
    "    file_path=pdf_path,\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    k=5,\n",
    "    alpha=0.5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "863df625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: The primary causes of climate change, as outlined in the context, are:\n",
      "\n",
      "1. **Increase in Greenhouse Gases**: The rise in greenhouse gases such as carbon dioxide (CO2), methane (CH4), and nitrous oxide (N2O) in the atmosphere, which trap heat from the sun and create a \"greenhouse effect.\"\n",
      "\n",
      "2. **Human Activities**: Significant contributions to climate change are attributed to human activities, particularly the burning of fossil fuels and deforestation.\n",
      "\n",
      "3. **Natural Variations**: Historically, climate changes have also been influenced by very small variations in Earth's orbit that affect the amount of solar energy received by the planet. \n",
      "\n",
      "These factors collectively drive the recent and unprecedented changes in the climate.\n",
      "Chunks used: 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the causes of climate change??\"\n",
    "\n",
    "answer, context = rag.query(question)\n",
    "print(f\"\\nAnswer: {answer}\")\n",
    "print(f\"Chunks used: {len(context)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28860ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
